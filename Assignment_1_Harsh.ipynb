{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xfXN9da_9oMv"
      },
      "source": [
        "### **Importing Libraries**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "VEhgNreByvTJ"
      },
      "outputs": [],
      "source": [
        "import itertools\n",
        "import math\n",
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import random\n",
        "\n",
        "from sklearn.model_selection  import train_test_split\n",
        "from sklearn.metrics import confusion_matrix as cnfm\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "from keras.datasets import fashion_mnist\n",
        "from keras.datasets import mnist\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BnoI6VGrHZ03",
        "outputId": "a6829ee2-f25f-40c6-c8ee-6293954cf2e1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: wandb in /usr/local/lib/python3.9/dist-packages (0.14.0)\n",
            "Requirement already satisfied: docker-pycreds>=0.4.0 in /usr/local/lib/python3.9/dist-packages (from wandb) (0.4.0)\n",
            "Requirement already satisfied: pathtools in /usr/local/lib/python3.9/dist-packages (from wandb) (0.1.2)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.9/dist-packages (from wandb) (63.4.3)\n",
            "Requirement already satisfied: GitPython!=3.1.29,>=1.0.0 in /usr/local/lib/python3.9/dist-packages (from wandb) (3.1.31)\n",
            "Requirement already satisfied: appdirs>=1.4.3 in /usr/local/lib/python3.9/dist-packages (from wandb) (1.4.4)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.9/dist-packages (from wandb) (6.0)\n",
            "Requirement already satisfied: protobuf!=4.21.0,<5,>=3.15.0 in /usr/local/lib/python3.9/dist-packages (from wandb) (3.19.6)\n",
            "Requirement already satisfied: setproctitle in /usr/local/lib/python3.9/dist-packages (from wandb) (1.3.2)\n",
            "Requirement already satisfied: Click!=8.0.0,>=7.0 in /usr/local/lib/python3.9/dist-packages (from wandb) (8.1.3)\n",
            "Requirement already satisfied: requests<3,>=2.0.0 in /usr/local/lib/python3.9/dist-packages (from wandb) (2.27.1)\n",
            "Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.9/dist-packages (from wandb) (5.9.4)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.9/dist-packages (from wandb) (4.5.0)\n",
            "Requirement already satisfied: sentry-sdk>=1.0.0 in /usr/local/lib/python3.9/dist-packages (from wandb) (1.17.0)\n",
            "Requirement already satisfied: six>=1.4.0 in /usr/local/lib/python3.9/dist-packages (from docker-pycreds>=0.4.0->wandb) (1.15.0)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.9/dist-packages (from GitPython!=3.1.29,>=1.0.0->wandb) (4.0.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests<3,>=2.0.0->wandb) (2022.12.7)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests<3,>=2.0.0->wandb) (3.4)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests<3,>=2.0.0->wandb) (1.26.15)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.9/dist-packages (from requests<3,>=2.0.0->wandb) (2.0.12)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.9/dist-packages (from gitdb<5,>=4.0.1->GitPython!=3.1.29,>=1.0.0->wandb) (5.0.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install wandb\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "jPyTqShZHo7Q"
      },
      "outputs": [],
      "source": [
        "import wandb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UnOMsHbSJi5a",
        "outputId": "40ce89f5-9ad0-4023-ad43-74c57afa79c0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ],
      "source": [
        "wandb.login(key='17d991db26320e751b170877037d1067a164fe6d')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 125
        },
        "id": "YalIxS34HZEP",
        "outputId": "f5161022-f7e3-4335-a52f-900a8bee85b3"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.14.0"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20230318_140254-gs7wa5c4</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/harshvrma/Assignment_1_finals1114/runs/gs7wa5c4' target=\"_blank\">ethereal-tree-4</a></strong> to <a href='https://wandb.ai/harshvrma/Assignment_1_finals1114' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/harshvrma/Assignment_1_finals1114' target=\"_blank\">https://wandb.ai/harshvrma/Assignment_1_finals1114</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/harshvrma/Assignment_1_finals1114/runs/gs7wa5c4' target=\"_blank\">https://wandb.ai/harshvrma/Assignment_1_finals1114/runs/gs7wa5c4</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/harshvrma/Assignment_1_finals1114/runs/gs7wa5c4?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>"
            ],
            "text/plain": [
              "<wandb.sdk.wandb_run.Run at 0x7efbc0387520>"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ],
      "source": [
        "wandb.init(project=\"Assignment_1_finals1114\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "IFpT3taysr9y"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "boR4MzKrsu5r"
      },
      "source": [
        "# **Question 1**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "QDT8vnMIssBT"
      },
      "outputs": [],
      "source": [
        "(train_data, train_class), (test_data, test_class) = fashion_mnist.load_data()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 373,
          "referenced_widgets": [
            "a22208e638ac48279d23273b12670d4c",
            "2c22d6bbaea3480b83dc6d792b821290",
            "571211911d7d4669bd39cee480f13f71",
            "bcbc7a2cf66e4885ad123040ca9258a7",
            "fcec2d4cbaf346a5a56cc7a28c8f973a",
            "5ff680f9048c45548160cc30bb2449cc",
            "e16cbadde22d45de9f9d1487c606b4ee",
            "1439ca20a2dc4d5b8c175828ac2fb32d"
          ]
        },
        "id": "6JUwCwuissF4",
        "outputId": "315ae471-c52a-40c1-a412-14c241318d13"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "VBox(children=(Label(value='0.006 MB of 0.014 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=0.428646…"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "a22208e638ac48279d23273b12670d4c"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">ethereal-tree-4</strong> at: <a href='https://wandb.ai/harshvrma/Assignment_1_finals1114/runs/gs7wa5c4' target=\"_blank\">https://wandb.ai/harshvrma/Assignment_1_finals1114/runs/gs7wa5c4</a><br/>Synced 4 W&B file(s), 10 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20230318_140254-gs7wa5c4/logs</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 10 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "dress=[\"T-shirt/top\", \"Trouser\",\"Pullover\", \"Dress\", \"Coat\", \"Sandal\", \"Shirt\", \"Sneaker\", \"Bag\",\"Ankle boot\"]\n",
        "\n",
        "\n",
        "unique=[]\n",
        "outputs = 10\n",
        "sizex = len(train_class)\n",
        "for i in range(outputs):\n",
        "  for j in range(sizex):\n",
        "    res = train_class[j]\n",
        "    if res==i:\n",
        "      ans = train_data[j]\n",
        "      unique.append(ans)\n",
        "      break;\n",
        "\n",
        "for i in range(len(unique)):\n",
        "  plt.subplot(2,5,i+1)\n",
        "  plt.imshow(unique[i])\n",
        "  plt.title(dress[i])\n",
        "wandb.run.name=\"Question-1\"\n",
        "wandb.log({\"imagess\":[wandb.Image(img,caption=item) for img,item in zip(unique,dress)]})\n",
        "wandb.finish()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HysJpdAgtAU9"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6fbKqafltA0Q"
      },
      "source": [
        "# **Question 2**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UAwOPQYJssKz"
      },
      "outputs": [],
      "source": [
        "\n",
        "train_data = np.array(train_data)\n",
        "train_data = train_data / 255.0\n",
        "test_data = np.array(test_data)\n",
        "test_data = test_data / 255.0\n",
        "print(train_data.shape)\n",
        "print(test_data.shape)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "IgNjaHXzssPm"
      },
      "outputs": [],
      "source": [
        "split = 0.2\n",
        "x_train, x_val, y_train, y_val = train_test_split(train_data, train_class, test_size=split, random_state=42)\n",
        "y_tr=y_train\n",
        "\n",
        "y_train_unencoded = y_train\n",
        "#One hot encoding of the class labels\n",
        "encoder = OneHotEncoder()\n",
        "y_train = encoder.fit_transform(np.expand_dims(y_train,1)).toarray()\n",
        "y_val = encoder.fit_transform(np.expand_dims(y_val,1)).toarray()\n",
        "y_test = encoder.fit_transform(np.expand_dims(test_class,1)).toarray()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "LghLEsUkssUe"
      },
      "outputs": [],
      "source": [
        "class FeedForwardNeuralNetwork():\n",
        "    def __init__(self,input_size,output_size,Y,hidden_layer=[128]):\n",
        "        self.num_inputs=input_size\n",
        "        self.num_outputs=output_size\n",
        "        self.hidden_sizes=len(hidden_layer)\n",
        "        self.hidden_layer=hidden_layer\n",
        "        self.Weights = {}\n",
        "        self.Biases = {}\n",
        "        self.sizes=[self.num_inputs]+hidden_layer+[self.num_outputs]                 \n",
        "            \n",
        "    \n",
        "    def softmax(self,x):\n",
        "        exps=np.exp(x)\n",
        "        res=exps/(np.sum(exps))\n",
        "        return res\n",
        "    def grad_softmax(y_pred,truey):\n",
        "            res =  y_pred-truey\n",
        "            return res\n",
        "    def tanh(self,x):\n",
        "        return np.tanh(x)\n",
        "    def derivative_tanh(x):\n",
        "        compute=np.tanh(x)**2\n",
        "        return 1-compute\n",
        "    def activation_function(self,name,x):\n",
        "        res = np.tanh(x)\n",
        "        return res\n",
        "    def forward_propogation(self,activation,X):\n",
        "        self.H={}\n",
        "        self.H[0]=X.reshape(1,-1)\n",
        "        self.A={}\n",
        "        for i in range(self.hidden_sizes):\n",
        "            wts = self.Weights[i+1]\n",
        "            self.A[i+1]=np.matmul(self.H[i],wts)+self.Biases[i+1]\n",
        "            self.H[i+1]=self.activation_function(activation,self.A[i+1])\n",
        "        self.A[(self.hidden_sizes)+1]=np.matmul(self.H[(self.hidden_sizes)],self.Weights[(self.hidden_sizes)+1])+self.Biases[(self.hidden_sizes)+1]\n",
        "        self.H[(self.hidden_sizes)+1]= self.softmax(self.A[len(self.hidden_layer)+1]) \n",
        "        return \n",
        "    def fit(self,activation,X,Y):\n",
        "        Y_pred=[]\n",
        "        for i in range(self.hidden_sizes+1):\n",
        "            current=self.sizes[i]\n",
        "            next_Layer=self.sizes[i+1]\n",
        "            self.Weights[i+1] = np.random.randn(current, next_Layer)\n",
        "            self.Biases[i+1] = np.zeros((1, next_Layer))\n",
        "        \n",
        "        for x,y in zip(X,Y):\n",
        "            self.forward_propogation(activation,x)\n",
        "            Y_pred.append(self.H[(self.hidden_sizes)+1][0])\n",
        "        return Y_pred\n",
        "        \n",
        "        \n",
        "    \n",
        "    \n",
        "    \n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "id": "Bicb8H1assZP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "504bb946-8da8-4b2d-c15b-c2bbd8330fc8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The probability distribution for Image Index 455 is \n",
            "\n",
            "[0.00133078 0.00000083 0.         0.00145337 0.99504449 0.00000005\n",
            " 0.00000002 0.00000691 0.00216356 0.        ]\n"
          ]
        }
      ],
      "source": [
        "xsize = x_train.shape[1]*x_train.shape[1]\n",
        "ysize = y_train.shape[1]\n",
        "hidden = [100,256,512]\n",
        "model = FeedForwardNeuralNetwork(xsize,ysize,hidden)\n",
        "activation_name = \"tanh\"\n",
        "class_predictions = model.fit(activation_name, x_train, y_train)\n",
        "\n",
        "np.set_printoptions(suppress=True)\n",
        "print(\"The probability distribution for Image Index 455 is \\n\")\n",
        "print(class_predictions[455])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "YFHakLd8sseJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "84b279ba-3c96-4451-97b8-7ea02bf2720b"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0.00020793, 0.9325292 , 0.        , 0.00000005, 0.        ,\n",
              "       0.        , 0.06726221, 0.        , 0.00000013, 0.00000048])"
            ]
          },
          "metadata": {},
          "execution_count": 38
        }
      ],
      "source": [
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hxOmlOSN-Cnh"
      },
      "source": [
        "### **`Question 3-9`**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GRPv16W2srX5"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "id": "T_RoS2jwgbmG"
      },
      "outputs": [],
      "source": [
        "(train_images, train_labels),(test_images, test_labels) = fashion_mnist.load_data()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "id": "_lUc5ltEQcM_"
      },
      "outputs": [],
      "source": [
        "train_images, val_images, train_labels, val_labels  = train_test_split(train_images,train_labels,test_size=0.1,random_state = 42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "id": "8fMVYaXPdYql"
      },
      "outputs": [],
      "source": [
        "trainsize=train_images.shape[0]\n",
        "traind = train_images.reshape(trainsize,-1)\n",
        "mean = traind.mean(axis=0)\n",
        "normalized = (traind -  mean)/np.max((traind -  mean).max(axis=0))\n",
        "\n",
        "valsize=val_images.shape[0]\n",
        "val_images = val_images.reshape(valsize,-1)\n",
        "mean = val_images.mean(axis=0)\n",
        "val_images =  (val_images -  mean)/np.max((val_images -  mean).max(axis=0))\n",
        "\n",
        "testsize=test_images.shape[0]\n",
        "test_images = test_images.reshape(testsize,-1)\n",
        "max = (test_images -  mean).max(axis=0)\n",
        "test_images = (test_images -  mean)/np.max((test_images -  mean).max(axis=0))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2ccGAXKwVIqW"
      },
      "source": [
        "### **Starting Of NN**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XuBSocl0XaZE"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dh3vmHAM59rh"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "id": "f3tWwkPE59we"
      },
      "outputs": [],
      "source": [
        "class NeuralNet:\n",
        "    def __init__(self,Size_of_Input, Number_of_Neuron_each_Layer, Number_of_Layers, activation_function, typeOfInit, L2reg_const = 0):\n",
        "        self.activation_function = activation_function\n",
        "        self.Size_of_Input = Size_of_Input\n",
        "        self.Number_of_Layers = Number_of_Layers\n",
        "        self.Number_of_Neuron_each_Layer = Number_of_Neuron_each_Layer\n",
        "        self.L2reg_const = L2reg_const\n",
        "        self.W,self.b = self.initializer(typeOfInit)\n",
        "\n",
        "    \n",
        "    def initializer(self, init):    \n",
        "        b = []    \n",
        "        W = []\n",
        "        \n",
        "        layers_size = self.Number_of_Layers\n",
        "        if (init == 'xavier'):\n",
        "            input_neuron = self.Number_of_Neuron_each_Layer[0]\n",
        "            parameter =  input_neuron+ self.Size_of_Input\n",
        "            input_size = self.Size_of_Input\n",
        "            number_of_layers = layers_size\n",
        "            W.append(np.random.normal(0,math.sqrt(2/(parameter)), ( input_neuron, input_size)))\n",
        "            for i in range(1,layers_size):\n",
        "                W.append(np.random.normal(0, math.sqrt(2/(self.Number_of_Neuron_each_Layer[i]+self.Number_of_Neuron_each_Layer[i-1])),(self.Number_of_Neuron_each_Layer[i],self.Number_of_Neuron_each_Layer[i-1])))\n",
        "\n",
        "            for i in range(number_of_layers):\n",
        "                curr_layer= self.Number_of_Neuron_each_Layer[i]\n",
        "                b.append(np.random.rand(curr_layer))\n",
        "        if init == 'random':\n",
        "            input_neuron = self.Number_of_Neuron_each_Layer[0]\n",
        "            input_size = self.Size_of_Input\n",
        "            W.append(np.random.randn(input_neuron, input_size))\n",
        "            number_of_layers = layers_size\n",
        "            for i in range(1,number_of_layers):\n",
        "                curr_layer = self.Number_of_Neuron_each_Layer[i]\n",
        "                prev_layer = self.Number_of_Neuron_each_Layer[i-1]\n",
        "                W.append(np.random.randn(curr_layer,prev_layer))\n",
        "\n",
        "            for i in range( number_of_layers):\n",
        "                curr_layer = self.Number_of_Neuron_each_Layer[i]\n",
        "                b.append(np.random.rand(curr_layer))\n",
        "        return W,b\n",
        "\n",
        "\n",
        "    def activation(self, Z):\n",
        "        if self.activation_function == 'ReLU':\n",
        "            res = self.ReLU(Z)\n",
        "            return res\n",
        "        elif self.activation_function == 'tanh':\n",
        "            res =  self.tanh(Z)\n",
        "            return res\n",
        "        elif self.activation_function == 'sigmoid':\n",
        "            res = self.sigmoid(Z)\n",
        "            return res\n",
        "\n",
        "\n",
        "    def activation_derivative(self,Z):\n",
        "        if self.activation_function == 'ReLU':\n",
        "            res = self.ReLU_derivative(Z)\n",
        "            return res\n",
        "        if self.activation_function == 'tanh':\n",
        "            res = self.tanh_derivative(Z)\n",
        "            return res\n",
        "        if self.activation_function == 'sigmoid':\n",
        "            res = self.sigmoid_derivative(Z)\n",
        "            return res\n",
        "    def ReLU(self,Z):\n",
        "        res= np.maximum(0,Z)\n",
        "        return res\n",
        "\n",
        "    def ReLU_derivative(self,Z):\n",
        "        res = [1 if x>0 else 0 for x in Z]\n",
        "        return res\n",
        "\n",
        "    def tanh(self, Z):\n",
        "        res = np.array([((np.exp(x) - np.exp(-x))/((np.exp(x) + np.exp(-x)))) for x in Z])\n",
        "        return res\n",
        "                 \n",
        "    def tanh_derivative(self, Z):\n",
        "        res = np.array(1 - self.tanh(Z)**2)\n",
        "        return res\n",
        "                 \n",
        "    def sigmoid_derivative(self,Z):\n",
        "        res = self.sigmoid(Z)*(1-self.sigmoid(Z))\n",
        "        return res\n",
        "\n",
        "    def sigmoid(self,x):\n",
        "        res = np.where(x>=0, 1/(1+np.exp(-x)), np.exp(x)/(1+np.exp(x)))\n",
        "        return res\n",
        "    \n",
        "    def softmax_function(self,Z):\n",
        "            maxZ=Z.max()\n",
        "            Z =   Z - maxZ# This is done to normalize the dataset\n",
        "            prob =np.exp(Z)\n",
        "            sumprob=np.sum(np.exp(Z),axis=0)\n",
        "            return (prob/sumprob)\n",
        "\n",
        "    def forward_propagation(self,Input):\n",
        "        Input = np.array(Input)\n",
        "        A = []\n",
        "        H = []\n",
        "        res = self.W[0].dot(Input) + self.b[0]\n",
        "        A.append(res)\n",
        "        number_of_layers =  self.Number_of_Layers\n",
        "        for i in range(1, number_of_layers):\n",
        "            H.append(self.activation(A[-1]))\n",
        "            preactivation = self.W[i].dot(H[-1]) + self.b[i]\n",
        "            A.append(preactivation)\n",
        "        y_hat = self.softmax_function(A[-1])\n",
        "        return A, H, y_hat\n",
        "\n",
        "    def backward_propagation(self, A, H, y_hat, y, Input, loss_type):\n",
        "        Input = np.array(Input)\n",
        "        H.insert(0,Input)\n",
        "        delW = []\n",
        "        delb = []\n",
        "        delA = []\n",
        "        delH = []\n",
        "\n",
        "        last_layer=self.Number_of_Neuron_each_Layer[-1]\n",
        "        zeros= np.zeros(last_layer)\n",
        "        ey = zeros\n",
        "        ey[y] = 1\n",
        "        \n",
        "        if loss_type==\"squared_error\":\n",
        "            der1=(y_hat - ey)\n",
        "            der2=(y_hat - y_hat**2)\n",
        "            delA.append(np.array(der1*der2))\n",
        "        else:\n",
        "        # delA and delH have reverse indexing\n",
        "            res=-(ey - y_hat)\n",
        "            delA.append(np.array(res))\n",
        "        number_of_layers= self.Number_of_Layers\n",
        "        for i in range( number_of_layers-1,-1,-1):\n",
        "            delastAshape=delA[-1].shape[0]\n",
        "            hlastshape=H[i].shape[0]\n",
        "            regulariz=self.L2reg_const*self.W[i]\n",
        "            delW.insert(0,delA[-1].reshape(delastAshape,1).dot(H[i].reshape(hlastshape,1).T) +regulariz )\n",
        "            delb.insert(0,delA[-1])\n",
        "            temp=self.W[i].T.dot(delA[-1])\n",
        "            delH.append(temp)\n",
        "            if i-1>=0:\n",
        "                delA.append(np.multiply(delH[-1], self.activation_derivative(A[i-1])))\n",
        "        return delW,delb\n",
        "    \n",
        "    \n",
        "    def initialize(self, Size_of_Input,Number_of_Layers,Number_of_Neuron_each_Layer):\n",
        "        W, b = [], []\n",
        "        each_layer_neuron = Number_of_Neuron_each_Layer[0]\n",
        "        input_size = Size_of_Input\n",
        "        W.append(np.zeros(( Number_of_Neuron_each_Layer[0],input_size )))\n",
        "        for i in range(1,Number_of_Layers):\n",
        "            curr_layer = Number_of_Neuron_each_Layer[i]\n",
        "            prev_layer = Number_of_Neuron_each_Layer[i-1]\n",
        "            W.append(np.zeros((curr_layer,prev_layer)))\n",
        "        for i in range(Number_of_Layers):\n",
        "            curr_layer = Number_of_Neuron_each_Layer[i]\n",
        "            b.append(np.zeros(curr_layer))            \n",
        "        return W, b\n",
        "\n",
        "    \n",
        "    def optimize(self, X, Y, val_images,val_labels,optimizer, learning_rate, max_epochs,batch_size,loss_type,momentum=0.5,beta = 0.89, epsilon = 1e-6,beta1 = 0.89,beta2 = 0.989):\n",
        "        if optimizer == 'sgd':\n",
        "          self.stochastic_gradient_descent(X, Y, val_images,val_labels, learning_rate, max_epochs,loss_type)\n",
        "        if optimizer == 'momentum':\n",
        "          self.momentum_gradient_descent(X, Y, val_images,val_labels, learning_rate, max_epochs,batch_size,loss_type)\n",
        "        if optimizer == 'nag':\n",
        "          self.nesterov_accelerated_gradient_descent(X, Y, val_images,val_labels, learning_rate, max_epochs,batch_size,loss_type)\n",
        "        if optimizer == 'rmsprop':\n",
        "          self.rmsprop(X, Y, val_images,val_labels, learning_rate, max_epochs,batch_size,loss_type)\n",
        "        if optimizer == 'adam':\n",
        "          self.adam(X, Y, val_images,val_labels, learning_rate, max_epochs,batch_size,loss_type)\n",
        "        if optimizer == 'nadam':\n",
        "          self.nadam(X, Y, val_images,val_labels, learning_rate, max_epochs,batch_size,loss_type)\n",
        "\n",
        "\n",
        "    def stochastic_gradient_descent(self,X, Y, val_images,val_labels, learning_rate, max_epochs,loss_type):\n",
        "        number_of_inputs = self.Size_of_Input\n",
        "        layers_size = self.Number_of_Layers\n",
        "        neurons = self.Number_of_Neuron_each_Layer\n",
        "        for j in range(max_epochs):\n",
        "            correct = 0\n",
        "            error = 0\n",
        "            delW, delb = self.initialize(number_of_inputs,layers_size,neurons)\n",
        "            xsize=X.shape[0]\n",
        "            for i in range(xsize):\n",
        "                features=X[i]\n",
        "                A,H,y_hat = self.forward_propagation(features)\n",
        "                upW=self.W\n",
        "                s = [x.sum() for x in upW]\n",
        "                zeros=np.zeros(self.Number_of_Neuron_each_Layer[-1])\n",
        "                ey = zeros\n",
        "                ey[Y[i]] = 1\n",
        "                if loss_type == \"squared_error\":\n",
        "                      sqrd = 0.5*np.sum((ey-y_hat)**2)\n",
        "                      reg =  self.L2reg_const/2*sum(s)\n",
        "                      error += sqrd + reg \n",
        "                else:\n",
        "                      cross =-math.log(y_hat[Y[i]])\n",
        "                      reg = self.L2reg_const/2*sum(s)\n",
        "                      error += cross + reg\n",
        "\n",
        "                delW,delb = self.backward_propagation(A,H,y_hat,Y[i],X[i],loss_type)\n",
        "                res=np.argmax(y_hat)\n",
        "                if(res == Y[i]):\n",
        "                    correct +=1\n",
        "                \n",
        "                for i in range(self.Number_of_Layers):\n",
        "                    lrdelw = learning_rate*delW[i]\n",
        "                    self.W[i] = self.W[i] - lrdelw\n",
        "                    lrdelb = learning_rate*delb[i]\n",
        "                    self.b[i] = self.b[i] - lrdelb\n",
        "\n",
        "            v_error, v_accruracy = self.val_loss_and_accuracy(val_images, val_labels,loss_type)\n",
        "            sizex=X.shape[0]\n",
        "            error =error/sizex\n",
        "            sizexacc= X.shape[0]\n",
        "            accuracy = correct/sizexacc*100\n",
        "            wandb.log({'epoch' : j, 'train_loss' : error, 'train_accuracy' : accuracy,'valid_loss' : v_error,'valid_accuracy' : v_accruracy})\n",
        "\n",
        "\n",
        "    def momentum_gradient_descent(self,X, Y, val_images,val_labels, learning_rate, max_epochs,batch_size, loss_type,gamma = 0.6):\n",
        "        number_of_input=self.Size_of_Input\n",
        "        number_of_layers=self.Number_of_Layers\n",
        "        neuron=self.Number_of_Neuron_each_Layer\n",
        "        updateW, updateb = self.initialize(number_of_input,number_of_layers,neuron)\n",
        "\n",
        "        for j in range(max_epochs):\n",
        "            correct = 0\n",
        "            error = 0\n",
        "\n",
        "            delW, delb = self.initialize(number_of_input, number_of_layers,neuron)\n",
        "            xsize=X.shape[0]\n",
        "            \n",
        "            for i in range(X.shape[0]):\n",
        "                features=X[i]\n",
        "                A,H,y_hat = self.forward_propagation(features)\n",
        "                upW=self.W\n",
        "                s = [x.sum() for x in upW]\n",
        "                ey = np.zeros(self.Number_of_Neuron_each_Layer[-1])\n",
        "                ey[Y[i]] = 1\n",
        "                if loss_type == \"squared_error\":\n",
        "                      sqrd = 0.5*np.sum((ey-y_hat)**2)\n",
        "                      reg =  self.L2reg_const/2*sum(s)\n",
        "                      error += sqrd + reg \n",
        "                else : \n",
        "                     cross = -math.log(y_hat[Y[i]])\n",
        "                     reg = self.L2reg_const/2*sum(s)\n",
        "                     error += cross + reg\n",
        "                \n",
        "                w,b = self.backward_propagation(A,H,y_hat,Y[i],X[i],loss_type)\n",
        "\n",
        "                for k in range( number_of_layers):\n",
        "                    prevdelW = delW[k]\n",
        "                    delW[k]  = prevdelW + w[k]\n",
        "                    prevdelB = delb[k]\n",
        "                    delb[k]  = prevdelB + b[k]\n",
        "\n",
        "                for k in range( number_of_layers):\n",
        "                    gammaupW = gamma*updateW[k]\n",
        "                    lrdelw = learning_rate*delW[k]\n",
        "                    updateW[k] = gammaupW + learning_rate*delW[k]  \n",
        "\n",
        "                    gammaupB = gamma*updateb[k]\n",
        "                    lrdelb = learning_rate*delb[k]\n",
        "                    updateb[k] = gammaupB + lrdelb\n",
        "                temp=i%batch_size\n",
        "                if  (temp == 0 and i!=0) or i==xsize-1:\n",
        "                    delW, delb = self.initialize(number_of_input, number_of_layers,neuron)\n",
        "                    for k in range( number_of_layers):\n",
        "                        prevW1 = self.W[k] \n",
        "                        self.W[k] = prevW1 -updateW[k]\n",
        "                        prevB1 = self.b[k]  \n",
        "                        self.b[k] = prevB1 -updateb[k]\n",
        "                res = np.argmax(y_hat)\n",
        "                if(res == Y[i]):\n",
        "                    correct +=1\n",
        "\n",
        "                \n",
        "            v_error, v_accruracy = self.val_loss_and_accuracy(val_images, val_labels,loss_type)\n",
        "            sizex=X.shape[0]\n",
        "            error =error/sizex\n",
        "            sizexacc= X.shape[0]\n",
        "            accuracy = correct/sizexacc*100\n",
        "            wandb.log({'epoch' : j, 'train_loss' : error, 'train_accuracy' : accuracy,'valid_loss' : v_error,'valid_accuracy' : v_accruracy})\n",
        "\n",
        "\n",
        "    def nesterov_accelerated_gradient_descent(self, X, Y, val_images,val_labels, learning_rate, max_epochs,batch_size,loss_type ,gamma = 0.5):\n",
        "        number_of_input= self.Size_of_Input\n",
        "        layers_size=self.Number_of_Layers\n",
        "        neurons=self.Number_of_Neuron_each_Layer\n",
        "        updateW, updateb = self.initialize( number_of_input, layers_size,neurons)\n",
        "        lookaheadW, lookaheadb = self.initialize( number_of_input,  layers_size,neurons)\n",
        "        thetaW, thetab = self.initialize( number_of_input, layers_size,neurons)\n",
        "\n",
        "        for j in range(max_epochs):\n",
        "            correct = 0\n",
        "            error = 0\n",
        "\n",
        "            delW, delb = self.initialize(number_of_input,layers_size,neurons)\n",
        "                \n",
        "            for k in range(layers_size):\n",
        "                thetaW[k] = self.W[k]\n",
        "                thetab[k] = self.b[k]\n",
        "\n",
        "            for k in range(layers_size):\n",
        "                gammaupW=gamma*updateW[k]\n",
        "                lookaheadW[k] = thetaW[k] - gammaupW \n",
        "                gammaupB=gamma*updateb[k]\n",
        "                lookaheadb[k] = thetab[k] - gammaupB\n",
        "                self.W[k] = lookaheadW[k]\n",
        "                self.b[k] = lookaheadb[k]\n",
        "\n",
        "            xsize=X.shape[0]\n",
        "            for i in range(xsize):\n",
        "                A,H,y_hat = self.forward_propagation(X[i])\n",
        "                Wup=self.W\n",
        "                s = [x.sum() for x in Wup ]\n",
        "               # cross=-math.log(y_hat[Y[i]])\n",
        "               # regu= self.L2reg_const/2*sum(s)\n",
        "                #error += cross + regu\n",
        "                ey = np.zeros(self.Number_of_Neuron_each_Layer[-1])\n",
        "                ey[Y[i]] = 1\n",
        "                if loss_type == \"squared_error\":\n",
        "                      sqrd = 0.5*np.sum((ey-y_hat)**2)\n",
        "                      reg =  self.L2reg_const/2*sum(s)\n",
        "                      error += sqrd + reg \n",
        "                else : \n",
        "                    cross = -math.log(y_hat[Y[i]])\n",
        "                    reg = self.L2reg_const/2*sum(s)\n",
        "                    error += cross + reg\n",
        "                w,b = self.backward_propagation(A,H,y_hat,Y[i],X[i],loss_type)\n",
        "\n",
        "                for k in range(layers_size):\n",
        "                    prevdW=delW[k]\n",
        "                    delW[k] =prevdW + w[k]\n",
        "                    prevdB=delb[k]\n",
        "                    delb[k] =prevdB + b[k]\n",
        "\n",
        "                for k in range( layers_size):\n",
        "                    gammaW = gamma*updateW[k]\n",
        "                    lrdelW= learning_rate*delW[k] \n",
        "                    updateW[k] =  gammaW + lrdelW\n",
        "\n",
        "                    gammab =   gamma*updateb[k]\n",
        "                    lrdelb=    learning_rate*delb[k]\n",
        "                    updateb[k] = gammab +  lrdelb\n",
        "\n",
        "                temp=i%batch_size\n",
        "                if  (temp == 0 and i!=0) or i==xsize-1:\n",
        "                    delW, delb = self.initialize(self.Size_of_Input,layers_size,self.Number_of_Neuron_each_Layer)\n",
        "                    for k in range(layers_size):\n",
        "                        befW = self.W[k]\n",
        "                        self.W[k] = befW -updateW[k] \n",
        "                        befB = self.b[k]\n",
        "                        self.b[k] =befB -updateb[k]\n",
        "                res=np.argmax(y_hat)\n",
        "                if(res == Y[i]):\n",
        "                    correct +=1\n",
        "            \n",
        "           # error /= X.shape[0]\n",
        "           # accuracy = correct/X.shape[0]*100\n",
        "           # v_error, v_accruracy = self.val_loss_and_accuracy(val_images, val_labels)\n",
        "            v_error, v_accruracy = self.val_loss_and_accuracy(val_images, val_labels,loss_type)\n",
        "            sizex=X.shape[0]\n",
        "            error =error/sizex\n",
        "            sizexacc= X.shape[0]\n",
        "            accuracy = correct/sizexacc*100\n",
        "            wandb.log({'epoch' : j, 'train_loss' : error, 'train_accuracy' : accuracy,'valid_loss' : v_error,'valid_accuracy' : v_accruracy})\n",
        "\n",
        "\n",
        "\n",
        "    def rmsprop(self,X, Y, val_images,val_labels, learning_rate, max_epochs,batch_size,loss_type, beta = 0.89, epsilon = 1e-6):\n",
        "        number_of_inputs=self.Size_of_Input\n",
        "        layers_size=self.Number_of_Layers\n",
        "        neurons=self.Number_of_Neuron_each_Layer\n",
        "        v_W, v_b = self.initialize(number_of_inputs,layers_size,neurons)\n",
        "\n",
        "        for j in range(max_epochs):\n",
        "            error = 0\n",
        "            correct = 0\n",
        "            xsize=X.shape[0]\n",
        "\n",
        "            delW, delb = self.initialize(number_of_inputs,layers_size,neurons)\n",
        "    \n",
        "            for i in range(xsize):\n",
        "                val= X[i]\n",
        "                A,H,y_hat = self.forward_propagation(val)\n",
        "                Wup=self.W\n",
        "                s = [x.sum() for x in Wup ]\n",
        "                #cross= -math.log(y_hat[Y[i]])\n",
        "               # reg= self.L2reg_const/2*sum(s)\n",
        "                #error += cross  + reg\n",
        "                ey = np.zeros(self.Number_of_Neuron_each_Layer[-1])\n",
        "                ey[Y[i]] = 1\n",
        "                if loss_type == \"squared_error\":\n",
        "                      sqrd = 0.5*np.sum((ey-y_hat)**2)\n",
        "                      reg =  self.L2reg_const/2*sum(s)\n",
        "                      error += sqrd + reg \n",
        "                else : \n",
        "                    cross = -math.log(y_hat[Y[i]])\n",
        "                    reg = self.L2reg_const/2*sum(s)\n",
        "                    error += cross + reg\n",
        "\n",
        "                w,b = self.backward_propagation(A,H,y_hat,Y[i],X[i],loss_type)\n",
        "\n",
        "                for k in range(layers_size):\n",
        "                    prevdelW = delW[k]\n",
        "                    delW[k]  = prevdelW + w[k]\n",
        "                    prevdelB = delb[k]\n",
        "                    delb[k]  = prevdelB +  b[k]\n",
        "                res=np.argmax(y_hat)\n",
        "                if(res == Y[i]):\n",
        "                    correct +=1\n",
        "\n",
        "                for k in range(layers_size):\n",
        "                    betavw1=beta*v_W[k]\n",
        "                    betavw2=(1-beta)*delW[k]**2 \n",
        "                    v_W[k] = betavw1  + betavw2 \n",
        "\n",
        "                    betavb1=  beta*v_b[k]\n",
        "                    betavb2=  (1-beta)*delb[k]**2 \n",
        "                    v_b[k] = betavb1 + betavb2\n",
        "         \n",
        "                temp= i%batch_size\n",
        "                if  (temp== 0 and i!=0) or i==xsize-1:\n",
        "                    for k in range(layers_size):\n",
        "                        betavw1= beta*v_W[k]\n",
        "                        betavw2= (1-beta)*delW[k]**2  \n",
        "                        v_W[k] =  betavw1 +  betavw2\n",
        "\n",
        "                        betavb1=  beta*v_b[k]   \n",
        "                        betavb2=  (1-beta)*delb[k]**2\n",
        "                        v_b[k] = betavb1 + betavb2\n",
        "                    for k in range(layers_size):\n",
        "                        lrdelw=(learning_rate*delW[k])\n",
        "                        sqrtvW=np.sqrt(v_W[k] + epsilon)\n",
        "                        self.W[k] = self.W[k] - lrdelw/sqrtvW\n",
        "\n",
        "                        lrdelb=(learning_rate*delb[k])\n",
        "                        sqrtvB=np.sqrt(v_b[k] + epsilon)\n",
        "                        self.b[k] = self.b[k] - lrdelb/sqrtvB\n",
        "                    delW, delb = self.initialize(number_of_inputs,layers_size,neurons) \n",
        "            \n",
        "\n",
        "            v_error, v_accruracy = self.val_loss_and_accuracy(val_images, val_labels,loss_type)\n",
        "            sizex=X.shape[0]\n",
        "            error =error/sizex\n",
        "            sizexacc= X.shape[0]\n",
        "            accuracy = correct/sizexacc*100\n",
        "            wandb.log({'epoch' : j, 'train_loss' : error, 'train_accuracy' : accuracy,'valid_loss' : v_error,'valid_accuracy' : v_accruracy})\n",
        "\n",
        "\n",
        "    \n",
        "    def adam(self,X, Y, val_images,val_labels, learning_rate, max_epochs,batch_size, loss_type,beta1 = 0.89,beta2 = 0.989,epsilon = 1e-8):\n",
        "        number_of_input = self.Size_of_Input\n",
        "        number_of_layers= self.Number_of_Layers\n",
        "        neurons= self.Number_of_Neuron_each_Layer\n",
        "        m_W, m_b = self.initialize( number_of_input,number_of_layers,neurons)\n",
        "        m_hat_W, m_hat_b = self.initialize(  number_of_input,number_of_layers,neurons)\n",
        "        v_W, v_b = self.initialize( number_of_input,number_of_layers,neurons)\n",
        "        v_hat_W, v_hat_b = self.initialize( number_of_input,number_of_layers,neurons)\n",
        "        \n",
        "        for j in range(0, max_epochs):\n",
        "            correct = 0\n",
        "            error = 0\n",
        "            xsize = X.shape[0]\n",
        "            delW, delb = self.initialize( number_of_input,number_of_layers,neurons)\n",
        "            \n",
        "            for i in range(xsize):\n",
        "                features=X[i]\n",
        "                A,H,y_hat = self.forward_propagation(features)\n",
        "                upW = self.W\n",
        "                s = [x.sum() for x in upW]\n",
        "               # cross = -math.log(y_hat[Y[i]])\n",
        "                #reg = self.L2reg_const/2*sum(s)\n",
        "               # error += cross + reg\n",
        "                ey = np.zeros(self.Number_of_Neuron_each_Layer[-1])\n",
        "                ey[Y[i]] = 1\n",
        "                if loss_type == \"squared_error\":\n",
        "                      sqrd = 0.5*np.sum((ey-y_hat)**2)\n",
        "                      reg =  self.L2reg_const/2*sum(s)\n",
        "                      error += sqrd + reg \n",
        "                else : \n",
        "                    cross = -math.log(y_hat[Y[i]])\n",
        "                    reg = self.L2reg_const/2*sum(s)\n",
        "                    error += cross + reg\n",
        "               \n",
        "\n",
        "                w,b = self.backward_propagation(A,H,y_hat,Y[i],X[i],loss_type)\n",
        "\n",
        "                for k in range(number_of_layers):\n",
        "                    prevdelW = delW[k]\n",
        "                    delW[k] = prevdelW + w[k]\n",
        "                    prevdelB = delb[k]\n",
        "                    delb[k] = prevdelB + b[k]\n",
        "                res = np.argmax(y_hat) \n",
        "                if(res == Y[i]):\n",
        "                    correct +=1\n",
        "                temp = i%batch_size\n",
        "                if  (temp == 0 and i!=0) or i==xsize-1:\n",
        "                    for k in range(number_of_layers):\n",
        "                        betavW1 =  beta2*v_W[k]\n",
        "                        betavW2 = (1-beta2)*delW[k]*delW[k]\n",
        "                        v_W[k] =   betavW1 + betavW2\n",
        "\n",
        "                        betavB1 = beta2*v_b[k]\n",
        "                        betavB2=  (1-beta2)*delb[k]*delb[k]\n",
        "                        v_b[k] =  betavB1 + betavB2\n",
        "\n",
        "                        betamW1 = beta1*m_W[k]\n",
        "                        betadelw =  (1-beta1)*delW[k]\n",
        "                        m_W[k] = betamW1 + betadelw\n",
        "\n",
        "                        betamb1 = beta1*m_b[k]\n",
        "                        betadelb= beta1*m_b[k]\n",
        "                        m_b[k] = betamb1 + betadelb\n",
        "\n",
        "                        powbeta1= (math.pow(beta1,j))\n",
        "                        m_hat_W[k] = m_W[k]/ powbeta1\n",
        "                        m_hat_b[k] = m_b[k]/powbeta1\n",
        "\n",
        "                        powbeta2= (math.pow(beta2,j))\n",
        "                        v_hat_W[k] = v_W[k]/powbeta2\n",
        "                        v_hat_b[k] = v_b[k]/powbeta2\n",
        "                    \n",
        "                    for k in range(number_of_layers):\n",
        "                        lrW = learning_rate*m_hat_W[k]\n",
        "                        sqrtvW = np.sqrt(v_hat_W[k] + epsilon) \n",
        "                        self.W[k] = self.W[k] - (lrW)/sqrtvW\n",
        "\n",
        "                        lrb = learning_rate*m_hat_b[k]\n",
        "                        sqrtvb = np.sqrt(v_hat_b[k] + epsilon)\n",
        "                        self.b[k] = self.b[k] - (lrb)/sqrtvb\n",
        "                    delW, delb = self.initialize(number_of_input,number_of_layers,neurons)\n",
        "                                \n",
        "            v_error, v_accruracy = self.val_loss_and_accuracy(val_images, val_labels,loss_type)\n",
        "            sizex=X.shape[0]\n",
        "            error =error/sizex\n",
        "            sizexacc= X.shape[0]\n",
        "            accuracy = correct/sizexacc*100\n",
        "            wandb.log({'epoch' : j, 'train_loss' : error, 'train_accuracy' : accuracy,'valid_loss' : v_error,'valid_accuracy' : v_accruracy})\n",
        "    \n",
        "    def nadam(self, X, Y, val_images,val_labels, learning_rate, max_epochs,batch_size, loss_type,beta1 = 0.89,beta2 = 0.989,epsilon = 1e-8):\n",
        "        number_of_inputs=self.Size_of_Input\n",
        "        layers_size=self.Number_of_Layers\n",
        "        neurons=self.Number_of_Neuron_each_Layer\n",
        "        v_W, v_b = self.initialize(number_of_inputs,layers_size,neurons)\n",
        "        v_hat_W, v_hat_b = self.initialize(number_of_inputs,layers_size,neurons)\n",
        "        m_W, m_b = self.initialize(number_of_inputs,layers_size,neurons)\n",
        "        m_hat_W, m_hat_b = self.initialize(number_of_inputs,layers_size,neurons)\n",
        "\n",
        "        \n",
        "        for j in range(1,max_epochs):\n",
        "            error = 0\n",
        "            correct = 0\n",
        "\n",
        "            sizex=X.shape[0]\n",
        "            delW, delb = self.initialize(self.Size_of_Input,self.Number_of_Layers,self.Number_of_Neuron_each_Layer)\n",
        "            \n",
        "            for i in range(sizex):\n",
        "                A,H,y_hat = self.forward_propagation(X[i])\n",
        "                \n",
        "                s = [x.sum() for x in self.W]\n",
        "              # # crosse=-math.log(y_hat[Y[i]])\n",
        "                #regu=self.L2reg_const/2*sum(s)\n",
        "               # error += crosse + regu\n",
        "                ey = np.zeros(self.Number_of_Neuron_each_Layer[-1])\n",
        "                ey[Y[i]] = 1\n",
        "                if loss_type == \"squared_error\":\n",
        "                      sqrd = 0.5*np.sum((ey-y_hat)**2)\n",
        "                      reg =  self.L2reg_const/2*sum(s)\n",
        "                      error += sqrd + reg \n",
        "                else : \n",
        "                    cross = -math.log(y_hat[Y[i]])\n",
        "                    reg = self.L2reg_const/2*sum(s)\n",
        "                    error += cross + reg\n",
        "\n",
        "                w,b = self.backward_propagation(A,H,y_hat,Y[i],X[i],loss_type)\n",
        "\n",
        "                for k in range(layers_size):\n",
        "                    prevdelW= delW[k]\n",
        "                    delW[k] = prevdelW + w[k]\n",
        "                    prevdelB= delb[k]\n",
        "                    delb[k] = prevdelB+  b[k]\n",
        "                res=np.argmax(y_hat) \n",
        "                if(res== Y[i]):\n",
        "                    correct +=1\n",
        "                temp=i%batch_size\n",
        "\n",
        "                if  (temp == 0 and i!=0) or i==sizex-1:\n",
        "                    for k in range(layers_size):\n",
        "                        betavW1= beta2*v_W[k]\n",
        "                        betavW2= (1-beta2)*delW[k]**2\n",
        "                        v_W[k] =  betavW1 + betavW2\n",
        "\n",
        "                        betavB1=beta2*v_b[k]\n",
        "                        betavB2=(1-beta2)*delb[k]**2\n",
        "                        v_b[k] = betavB1  + betavB2 \n",
        "                        \n",
        "                        betamW1=beta1*m_W[k]\n",
        "                        betamW2=(1-beta1)*delW[k]\n",
        "                        m_W[k] = betamW1 + betamW2\n",
        "\n",
        "                        betamB1=beta1*m_b[k]\n",
        "                        betamB2=(1-beta1)*delb[k]\n",
        "                        m_b[k] = betamB1+ betamB2\n",
        "                        \n",
        "                        m_hat_bias= (math.pow(beta1,j))\n",
        "                        m_hat_W[k] = m_W[k]/m_hat_bias\n",
        "                        m_hat_b[k] = m_b[k]/m_hat_bias\n",
        "                        \n",
        "                        v_hat_bias = (math.pow(beta2,j))\n",
        "                        v_hat_W[k] = v_W[k]/v_hat_bias\n",
        "                        v_hat_b[k] = v_b[k]/v_hat_bias\n",
        "                    for k in range(self.Number_of_Layers):\n",
        "                        beta_mw=beta1*m_hat_W[k]\n",
        "                        beta_dw=(1-beta1)*delW[k]/(1-beta1)\n",
        "                        sqrt_dw=np.sqrt(v_hat_W[k] + epsilon)\n",
        "                        self.W[k] = self.W[k] - (learning_rate*(beta_mw + beta_dw))/sqrt_dw\n",
        "\n",
        "                        beta_mb=beta1*m_hat_b[k]\n",
        "                        beta_db=(1-beta1)*delb[k]/(1-beta1)\n",
        "                        sqrt_db=np.sqrt(v_hat_b[k] + epsilon)\n",
        "                        self.b[k] = self.b[k] - (learning_rate*(beta_mb+beta_db))/sqrt_db\n",
        "                    delW, delb = self.initialize(self.Size_of_Input,self.Number_of_Layers,self.Number_of_Neuron_each_Layer)\n",
        "                    \n",
        "\n",
        "            v_error, v_accruracy = self.val_loss_and_accuracy(val_images, val_labels,loss_type)\n",
        "            sizex=X.shape[0]\n",
        "            error =error/sizex\n",
        "            sizexacc= X.shape[0]\n",
        "            accuracy = correct/sizexacc*100\n",
        "            wandb.log({'epoch' : j, 'train_loss' : error, 'train_accuracy' : accuracy,'valid_loss' : v_error,'valid_accuracy' : v_accruracy})\n",
        "        \n",
        "    \n",
        "    def val_loss_and_accuracy(self,val_data,val_labels,loss_type):\n",
        "        val_loss = []\n",
        "        val_accuracy = []\n",
        "        val_correct = 0\n",
        "        val_error = 0\n",
        "\n",
        "        for i in range(val_data.shape[0]):\n",
        "            A,H,y_hat = self.forward_propagation(val_data[i]) \n",
        "            upW=self.W\n",
        "            s = [x.sum() for x in upW]\n",
        "            if loss_type == \"squared_error\":\n",
        "                ey = np.zeros(self.Number_of_Neuron_each_Layer[-1])\n",
        "                ey[val_labels[i]] = 1\n",
        "                error=0.5*np.sum((ey-y_hat)**2)\n",
        "                regerror= self.L2reg_const/2*sum(s)\n",
        "                val_error += error + regerror\n",
        "            else:\n",
        "                 regu=self.L2reg_const/2*sum(s)\n",
        "                 cross=-math.log(y_hat[val_labels[i]])\n",
        "                 val_error += cross + regu\n",
        "            maxi=np.argmax(y_hat)\n",
        "            if  maxi == val_labels[i]:\n",
        "                val_correct += 1\n",
        "        m=val_data.shape[0]\n",
        "        m1=val_data.shape[0]\n",
        "        return val_error/m, val_correct/m1*100\n",
        "\n",
        "\n",
        "    def test(self,test_data,test_labels):\n",
        "        correct = 0\n",
        "        y_hat = []\n",
        "        testsize=test_data.shape[0]\n",
        "        for i in range(testsize):\n",
        "\n",
        "            A,H,y = self.forward_propagation(test_data[i])\n",
        "            res=np.argmax(y)\n",
        "            if res == test_labels[i]:\n",
        "                correct += 1\n",
        "            y_hat.append(y)\n",
        "        a=np.argmax(np.array(y_hat),axis=1)\n",
        "        b=correct/test_data.shape[0]*100\n",
        "        return a,b"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sweep_config = {\n",
        "    'method': 'random',\n",
        "    'metric': {'goal': 'maximize', 'name': 'valid_accuracy'},\n",
        "    'parameters': {'activation_function': {'values': ['sigmoid', 'tanh', 'ReLU']},\n",
        "                'batch_size': {'values': [16, 32, 64]},\n",
        "                'epochs': {'values': [5,10]},\n",
        "                'hidden_layer_size': {'values': [32,64,128]},\n",
        "                'num_of_hidden_layers': { 'values' : [1,2] },\n",
        "                'learning_rate': {'values': [0.0006,\n",
        "                                             0.005,\n",
        "                                             0.0001,\n",
        "                                             0.0005,\n",
        "                                             0.0003,\n",
        "                                             0.01]},\n",
        "                'optimizer': {'values': ['sgd',\n",
        "                                         'momentum',\n",
        "                                         'nag',\n",
        "                                         'rmsprop',\n",
        "                                         'adam',\n",
        "                                         'nadam']},\n",
        "                'weight_decay': {'values': [0, 0.0005,0.5]},\n",
        "                'weight_initialization': {'values': ['random', 'xavier']},\n",
        "                'loss_type': {'values': ['squared_error', 'cross_entropy']}}}"
      ],
      "metadata": {
        "id": "FJ3PW_lnqdmx"
      },
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "id": "lFE8OTH8FB2r"
      },
      "outputs": [],
      "source": [
        "def train():\n",
        "    var1 = wandb.init()\n",
        "    var2 = var1.config\n",
        "    nsize = normalized.shape[1]\n",
        "    obj = NeuralNet(nsize, list(itertools.chain(*[[var2.hidden_layer_size]*var2.num_of_hidden_layers, [10]])), var2.num_of_hidden_layers+1, var2.activation_function, var2.weight_initialization, var2.weight_decay)\n",
        "    obj.optimize(normalized, train_labels, val_images, val_labels, var2.optimizer, var2.learning_rate, var2.epochs, var2.batch_size, var2.loss_type)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c1Fk-bEXFGD4",
        "outputId": "11b3ec27-2bf2-4492-a9cb-47970d07bcbc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Create sweep with ID: b4894dmx\n",
            "Sweep URL: https://wandb.ai/harshvrma/Assignment_1_finals1111111/sweeps/b4894dmx\n"
          ]
        }
      ],
      "source": [
        "\n",
        "\n",
        "sweep_id = wandb.sweep(sweep_config, project='Assignment_1_finals1111111')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {
        "id": "JeDcnDZaFNif",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "19e63f3f05b34249b12061aff63a6d82",
            "fa66be212a0b45ee88c0922b15f052e3",
            "e697f544c65149a1a0ba55c1968e90a3",
            "0247d18e7cea4eb7a9301e838bdbf45e",
            "80ccdbdd0b3541eb96635096d6b01f37",
            "61925964b39c424d98e06cdfd0801929",
            "b06a61c06d5c452cb2cd8924e96fd819",
            "f29d00ce976e42c489aa914094e8dc00"
          ]
        },
        "outputId": "b28b17e6-55b0-4d6e-b841-439af7a4b0dd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: erldbj0q with config:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tactivation_function: tanh\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 16\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 10\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \thidden_layer_size: 64\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.005\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tloss_type: squared_error\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_of_hidden_layers: 1\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer: rmsprop\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tweight_decay: 0\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tweight_initialization: xavier\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.14.0"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20230318_140718-erldbj0q</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/harshvrma/Assignment_1_finals1111111/runs/erldbj0q' target=\"_blank\">glad-sweep-1</a></strong> to <a href='https://wandb.ai/harshvrma/Assignment_1_finals1111111' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/harshvrma/Assignment_1_finals1111111/sweeps/b4894dmx' target=\"_blank\">https://wandb.ai/harshvrma/Assignment_1_finals1111111/sweeps/b4894dmx</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/harshvrma/Assignment_1_finals1111111' target=\"_blank\">https://wandb.ai/harshvrma/Assignment_1_finals1111111</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View sweep at <a href='https://wandb.ai/harshvrma/Assignment_1_finals1111111/sweeps/b4894dmx' target=\"_blank\">https://wandb.ai/harshvrma/Assignment_1_finals1111111/sweeps/b4894dmx</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/harshvrma/Assignment_1_finals1111111/runs/erldbj0q' target=\"_blank\">https://wandb.ai/harshvrma/Assignment_1_finals1111111/runs/erldbj0q</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<style>\n",
              "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
              "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
              "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
              "    </style>\n",
              "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▂▃▃▄▅▆▆▇█</td></tr><tr><td>train_accuracy</td><td>▁▇███▇██▇▇</td></tr><tr><td>train_loss</td><td>█▂▁▁▁▂▁▂▂▂</td></tr><tr><td>valid_accuracy</td><td>█▆▆▅▁▂▁▂▂▂</td></tr><tr><td>valid_loss</td><td>▁▃▁▃▆▆███▇</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>9</td></tr><tr><td>train_accuracy</td><td>84.99074</td></tr><tr><td>train_loss</td><td>0.11116</td></tr><tr><td>valid_accuracy</td><td>84.0</td></tr><tr><td>valid_loss</td><td>0.12027</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">glad-sweep-1</strong> at: <a href='https://wandb.ai/harshvrma/Assignment_1_finals1111111/runs/erldbj0q' target=\"_blank\">https://wandb.ai/harshvrma/Assignment_1_finals1111111/runs/erldbj0q</a><br/>Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20230318_140718-erldbj0q/logs</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: ve6m7lo0 with config:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tactivation_function: tanh\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 16\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 10\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \thidden_layer_size: 32\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.01\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tloss_type: cross_entropy\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_of_hidden_layers: 1\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer: rmsprop\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tweight_decay: 0\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tweight_initialization: xavier\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.14.0"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20230318_143446-ve6m7lo0</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/harshvrma/Assignment_1_finals1111111/runs/ve6m7lo0' target=\"_blank\">ethereal-sweep-2</a></strong> to <a href='https://wandb.ai/harshvrma/Assignment_1_finals1111111' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/harshvrma/Assignment_1_finals1111111/sweeps/b4894dmx' target=\"_blank\">https://wandb.ai/harshvrma/Assignment_1_finals1111111/sweeps/b4894dmx</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/harshvrma/Assignment_1_finals1111111' target=\"_blank\">https://wandb.ai/harshvrma/Assignment_1_finals1111111</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View sweep at <a href='https://wandb.ai/harshvrma/Assignment_1_finals1111111/sweeps/b4894dmx' target=\"_blank\">https://wandb.ai/harshvrma/Assignment_1_finals1111111/sweeps/b4894dmx</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/harshvrma/Assignment_1_finals1111111/runs/ve6m7lo0' target=\"_blank\">https://wandb.ai/harshvrma/Assignment_1_finals1111111/runs/ve6m7lo0</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<style>\n",
              "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
              "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
              "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
              "    </style>\n",
              "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▂▃▃▄▅▆▆▇█</td></tr><tr><td>train_accuracy</td><td>▁▅▅▆▆▇▇███</td></tr><tr><td>train_loss</td><td>█▅▄▃▃▂▂▂▂▁</td></tr><tr><td>valid_accuracy</td><td>▁▄▅▅▅▅▆█▇▇</td></tr><tr><td>valid_loss</td><td>█▅▃▃▅▁▃▃▅▂</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>9</td></tr><tr><td>train_accuracy</td><td>86.68333</td></tr><tr><td>train_loss</td><td>0.46077</td></tr><tr><td>valid_accuracy</td><td>85.25</td></tr><tr><td>valid_loss</td><td>0.50556</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">ethereal-sweep-2</strong> at: <a href='https://wandb.ai/harshvrma/Assignment_1_finals1111111/runs/ve6m7lo0' target=\"_blank\">https://wandb.ai/harshvrma/Assignment_1_finals1111111/runs/ve6m7lo0</a><br/>Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20230318_143446-ve6m7lo0/logs</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: cso4ejx9 with config:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tactivation_function: ReLU\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 16\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 10\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \thidden_layer_size: 64\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.0003\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tloss_type: squared_error\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_of_hidden_layers: 2\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer: adam\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tweight_decay: 0.5\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tweight_initialization: random\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.14.0"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20230318_144903-cso4ejx9</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/harshvrma/Assignment_1_finals1111111/runs/cso4ejx9' target=\"_blank\">cerulean-sweep-3</a></strong> to <a href='https://wandb.ai/harshvrma/Assignment_1_finals1111111' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/harshvrma/Assignment_1_finals1111111/sweeps/b4894dmx' target=\"_blank\">https://wandb.ai/harshvrma/Assignment_1_finals1111111/sweeps/b4894dmx</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/harshvrma/Assignment_1_finals1111111' target=\"_blank\">https://wandb.ai/harshvrma/Assignment_1_finals1111111</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View sweep at <a href='https://wandb.ai/harshvrma/Assignment_1_finals1111111/sweeps/b4894dmx' target=\"_blank\">https://wandb.ai/harshvrma/Assignment_1_finals1111111/sweeps/b4894dmx</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/harshvrma/Assignment_1_finals1111111/runs/cso4ejx9' target=\"_blank\">https://wandb.ai/harshvrma/Assignment_1_finals1111111/runs/cso4ejx9</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "VBox(children=(Label(value='0.001 MB of 0.008 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=0.118433…"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "19e63f3f05b34249b12061aff63a6d82"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<style>\n",
              "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
              "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
              "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
              "    </style>\n",
              "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▂▃▃▄▅▆▆▇█</td></tr><tr><td>train_accuracy</td><td>▃█▂▁▁▁▁▁▁▁</td></tr><tr><td>train_loss</td><td>▁▅████████</td></tr><tr><td>valid_accuracy</td><td>▆█▁▁▁▁▁▁▁▁</td></tr><tr><td>valid_loss</td><td>▁▇████████</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>9</td></tr><tr><td>train_accuracy</td><td>10.01481</td></tr><tr><td>train_loss</td><td>0.42423</td></tr><tr><td>valid_accuracy</td><td>9.95</td></tr><tr><td>valid_loss</td><td>0.48769</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">cerulean-sweep-3</strong> at: <a href='https://wandb.ai/harshvrma/Assignment_1_finals1111111/runs/cso4ejx9' target=\"_blank\">https://wandb.ai/harshvrma/Assignment_1_finals1111111/runs/cso4ejx9</a><br/>Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20230318_144903-cso4ejx9/logs</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: 6cj4vzsa with config:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tactivation_function: tanh\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 64\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 5\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \thidden_layer_size: 128\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.005\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tloss_type: cross_entropy\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_of_hidden_layers: 1\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer: nag\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tweight_decay: 0\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tweight_initialization: xavier\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.14.0"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20230318_145817-6cj4vzsa</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/harshvrma/Assignment_1_finals1111111/runs/6cj4vzsa' target=\"_blank\">proud-sweep-4</a></strong> to <a href='https://wandb.ai/harshvrma/Assignment_1_finals1111111' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/harshvrma/Assignment_1_finals1111111/sweeps/b4894dmx' target=\"_blank\">https://wandb.ai/harshvrma/Assignment_1_finals1111111/sweeps/b4894dmx</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/harshvrma/Assignment_1_finals1111111' target=\"_blank\">https://wandb.ai/harshvrma/Assignment_1_finals1111111</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View sweep at <a href='https://wandb.ai/harshvrma/Assignment_1_finals1111111/sweeps/b4894dmx' target=\"_blank\">https://wandb.ai/harshvrma/Assignment_1_finals1111111/sweeps/b4894dmx</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/harshvrma/Assignment_1_finals1111111/runs/6cj4vzsa' target=\"_blank\">https://wandb.ai/harshvrma/Assignment_1_finals1111111/runs/6cj4vzsa</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<style>\n",
              "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
              "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
              "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
              "    </style>\n",
              "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▃▅▆█</td></tr><tr><td>train_accuracy</td><td>▁▅▆▇█</td></tr><tr><td>train_loss</td><td>█▄▃▂▁</td></tr><tr><td>valid_accuracy</td><td>▁▆▇▇█</td></tr><tr><td>valid_loss</td><td>█▃▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>4</td></tr><tr><td>train_accuracy</td><td>89.6037</td></tr><tr><td>train_loss</td><td>0.28977</td></tr><tr><td>valid_accuracy</td><td>87.85</td></tr><tr><td>valid_loss</td><td>0.3536</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">proud-sweep-4</strong> at: <a href='https://wandb.ai/harshvrma/Assignment_1_finals1111111/runs/6cj4vzsa' target=\"_blank\">https://wandb.ai/harshvrma/Assignment_1_finals1111111/runs/6cj4vzsa</a><br/>Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20230318_145817-6cj4vzsa/logs</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: 32qkni9b with config:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tactivation_function: sigmoid\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 64\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 10\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \thidden_layer_size: 32\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.0006\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tloss_type: squared_error\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_of_hidden_layers: 2\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer: nadam\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tweight_decay: 0.5\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tweight_initialization: random\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.14.0"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20230318_152240-32qkni9b</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/harshvrma/Assignment_1_finals1111111/runs/32qkni9b' target=\"_blank\">peach-sweep-5</a></strong> to <a href='https://wandb.ai/harshvrma/Assignment_1_finals1111111' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/harshvrma/Assignment_1_finals1111111/sweeps/b4894dmx' target=\"_blank\">https://wandb.ai/harshvrma/Assignment_1_finals1111111/sweeps/b4894dmx</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/harshvrma/Assignment_1_finals1111111' target=\"_blank\">https://wandb.ai/harshvrma/Assignment_1_finals1111111</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View sweep at <a href='https://wandb.ai/harshvrma/Assignment_1_finals1111111/sweeps/b4894dmx' target=\"_blank\">https://wandb.ai/harshvrma/Assignment_1_finals1111111/sweeps/b4894dmx</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/harshvrma/Assignment_1_finals1111111/runs/32qkni9b' target=\"_blank\">https://wandb.ai/harshvrma/Assignment_1_finals1111111/runs/32qkni9b</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<style>\n",
              "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
              "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
              "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
              "    </style>\n",
              "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▂▃▄▅▅▆▇█</td></tr><tr><td>train_accuracy</td><td>█▁▂▂▂▁▂▁▁</td></tr><tr><td>train_loss</td><td>▁▄███████</td></tr><tr><td>valid_accuracy</td><td>█▁▄▃▃▃▃▃▃</td></tr><tr><td>valid_loss</td><td>▁▆███████</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>9</td></tr><tr><td>train_accuracy</td><td>9.71852</td></tr><tr><td>train_loss</td><td>0.44961</td></tr><tr><td>valid_accuracy</td><td>10.13333</td></tr><tr><td>valid_loss</td><td>0.44397</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">peach-sweep-5</strong> at: <a href='https://wandb.ai/harshvrma/Assignment_1_finals1111111/runs/32qkni9b' target=\"_blank\">https://wandb.ai/harshvrma/Assignment_1_finals1111111/runs/32qkni9b</a><br/>Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20230318_152240-32qkni9b/logs</code>"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "wandb.agent(sweep_id, train,count=5)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "var1 = wandb.init(project=\"Assignment_1_finals\")\n",
        "var2 = var1.config\n",
        "xsize=784\n",
        "output_size=10\n",
        "Batch_size=16\n",
        "hidden_layer=[32,32,10]\n",
        "number_of_hidden_layer=3\n",
        "obj = NeuralNet(xsize, hidden_layer, number_of_hidden_layer, 'ReLU', 'xavier', 0)\n",
        "optimizer='nadam'\n",
        "obj.optimize(normalized, train_labels, val_images, val_labels, optimizer, 0.0003,output_size, Batch_size,\"cross_entropy\")\n",
        "y_pred, test_accuracy = obj.test(test_images, test_labels)\n",
        "labels_all= [0,1,2,3,4,5,6,7,8,9]\n",
        "cm = cnfm(y_pred, test_labels, labels = labels_all)  \n",
        "value=False\n",
        "wandb.sklearn.plot_confusion_matrix(test_labels, y_pred, labels_all)\n",
        "wandb.log({'Confusion matrix': wandb.plots.HeatMap(labels_all, labels_all, cm, show_text=value), 'test_accuracy' : test_accuracy})"
      ],
      "metadata": {
        "id": "fC0DpMAyMXNn",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 191
        },
        "outputId": "c61e718f-69a3-4e48-8180-c1707dfa004c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Changes to your `wandb` environment variables will be ignored because your `wandb` session has already started. For more information on how to modify your settings with `wandb.init()` arguments, please refer to <a href='https://wandb.me/wandb-init' target=\"_blank\">the W&B docs</a>."
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignored wandb.init() arg project when running a sweep.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.14.0"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20230318_154148-32qkni9b</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/harshvrma/Assignment_1_finals1111111/runs/32qkni9b' target=\"_blank\">peach-sweep-5</a></strong> to <a href='https://wandb.ai/harshvrma/Assignment_1_finals1111111' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/harshvrma/Assignment_1_finals1111111/sweeps/b4894dmx' target=\"_blank\">https://wandb.ai/harshvrma/Assignment_1_finals1111111/sweeps/b4894dmx</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/harshvrma/Assignment_1_finals1111111' target=\"_blank\">https://wandb.ai/harshvrma/Assignment_1_finals1111111</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View sweep at <a href='https://wandb.ai/harshvrma/Assignment_1_finals1111111/sweeps/b4894dmx' target=\"_blank\">https://wandb.ai/harshvrma/Assignment_1_finals1111111/sweeps/b4894dmx</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/harshvrma/Assignment_1_finals1111111/runs/32qkni9b' target=\"_blank\">https://wandb.ai/harshvrma/Assignment_1_finals1111111/runs/32qkni9b</a>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "sdfbOJC_4AYI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "cdjXvQnOdi_F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "5oBqH0jTdjB9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **`Question 10`**"
      ],
      "metadata": {
        "id": "AUoWBVYwdpw1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "(train_images, train_labels),(test_images, test_labels) = mnist.load_data()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NlH7F4L6dmny",
        "outputId": "0f0f4f09-dc06-461b-81f5-f56151a1d22a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "11490434/11490434 [==============================] - 0s 0us/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_images, val_images, train_labels, val_labels  = train_test_split(train_images,train_labels,test_size=0.1,random_state = 42)"
      ],
      "metadata": {
        "id": "Z1wBkBAidxmk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainsize=train_images.shape[0]\n",
        "traind = train_images.reshape(trainsize,-1)\n",
        "mean = traind.mean(axis=0)\n",
        "normalized = (traind -  mean)/np.max((traind -  mean).max(axis=0))\n",
        "\n",
        "valsize=val_images.shape[0]\n",
        "val_images = val_images.reshape(valsize,-1)\n",
        "mean = val_images.mean(axis=0)\n",
        "val_images =  (val_images -  mean)/np.max((val_images -  mean).max(axis=0))\n",
        "\n",
        "testsize=test_images.shape[0]\n",
        "test_images = test_images.reshape(testsize,-1)\n",
        "max = (test_images -  mean).max(axis=0)\n",
        "test_images = (test_images -  mean)/np.max((test_images -  mean).max(axis=0))"
      ],
      "metadata": {
        "id": "cVb4CQrCd-NQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class NeuralNet:\n",
        "    def __init__(self,Size_of_Input, Number_of_Neuron_each_Layer, Number_of_Layers, activation_function, typeOfInit, L2reg_const = 0):\n",
        "        self.activation_function = activation_function\n",
        "        self.Size_of_Input = Size_of_Input\n",
        "        self.Number_of_Layers = Number_of_Layers\n",
        "        self.Number_of_Neuron_each_Layer = Number_of_Neuron_each_Layer\n",
        "        self.L2reg_const = L2reg_const\n",
        "        self.W,self.b = self.initializer(typeOfInit)\n",
        "\n",
        "    \n",
        "    def initializer(self, init):        \n",
        "        W = []\n",
        "        b = []\n",
        "        if init == 'random':\n",
        "            input_neuron = self.Number_of_Neuron_each_Layer[0]\n",
        "            input_size = self.Size_of_Input\n",
        "            W.append(np.random.randn(input_neuron, input_size))\n",
        "            number_of_layers = self.Number_of_Layers\n",
        "            for i in range(1,number_of_layers):\n",
        "                curr_layer = self.Number_of_Neuron_each_Layer[i]\n",
        "                prev_layer = self.Number_of_Neuron_each_Layer[i-1]\n",
        "                W.append(np.random.randn(curr_layer,prev_layer))\n",
        "\n",
        "            for i in range( number_of_layers):\n",
        "                curr_layer = self.Number_of_Neuron_each_Layer[i]\n",
        "                b.append(np.random.rand(curr_layer))\n",
        "        if (init == 'xavier'):\n",
        "            input_neuron = self.Number_of_Neuron_each_Layer[0]\n",
        "            parameter =  input_neuron+ self.Size_of_Input\n",
        "            input_size = self.Size_of_Input\n",
        "            number_of_layers = self.Number_of_Layers\n",
        "            W.append(np.random.normal(0,math.sqrt(2/(parameter)), ( input_neuron, input_size)))\n",
        "            for i in range(1,self.Number_of_Layers):\n",
        "                W.append(np.random.normal(0, math.sqrt(2/(self.Number_of_Neuron_each_Layer[i]+self.Number_of_Neuron_each_Layer[i-1])),(self.Number_of_Neuron_each_Layer[i],self.Number_of_Neuron_each_Layer[i-1])))\n",
        "\n",
        "            for i in range(number_of_layers):\n",
        "                curr_layer= self.Number_of_Neuron_each_Layer[i]\n",
        "                b.append(np.random.rand(curr_layer))\n",
        "        return W,b\n",
        "\n",
        "\n",
        "    def activation(self, Z):\n",
        "        if self.activation_function == 'ReLU':\n",
        "            res = self.ReLU(Z)\n",
        "            return res\n",
        "        elif self.activation_function == 'tanh':\n",
        "            res =  self.tanh(Z)\n",
        "            return res\n",
        "        elif self.activation_function == 'sigmoid':\n",
        "            res = self.sigmoid(Z)\n",
        "            return res\n",
        "\n",
        "\n",
        "    def activation_derivative(self,Z):\n",
        "        if self.activation_function == 'ReLU':\n",
        "            res = self.ReLU_derivative(Z)\n",
        "            return res\n",
        "        if self.activation_function == 'tanh':\n",
        "            res = self.tanh_derivative(Z)\n",
        "            return res\n",
        "        if self.activation_function == 'sigmoid':\n",
        "            res = self.sigmoid_derivative(Z)\n",
        "            return res\n",
        "    def ReLU(self,Z):\n",
        "        res= np.maximum(0,Z)\n",
        "        return res\n",
        "\n",
        "    def ReLU_derivative(self,Z):\n",
        "        res = [1 if x>0 else 0 for x in Z]\n",
        "        return res\n",
        "\n",
        "    def tanh(self, Z):\n",
        "        res = np.array([((np.exp(x) - np.exp(-x))/((np.exp(x) + np.exp(-x)))) for x in Z])\n",
        "        return res\n",
        "                 \n",
        "    def tanh_derivative(self, Z):\n",
        "        res = np.array(1 - self.tanh(Z)**2)\n",
        "        return res\n",
        "                 \n",
        "    def sigmoid_derivative(self,Z):\n",
        "        res = self.sigmoid(Z)*(1-self.sigmoid(Z))\n",
        "        return res\n",
        "\n",
        "    def sigmoid(self,x):\n",
        "        res = np.where(x>=0, 1/(1+np.exp(-x)), np.exp(x)/(1+np.exp(x)))\n",
        "        return res\n",
        "    \n",
        "    def softmax_function(self,Z):\n",
        "            maxZ=Z.max()\n",
        "            Z =   Z - maxZ# This is done to normalize the dataset\n",
        "            prob =np.exp(Z)\n",
        "            sumprob=np.sum(np.exp(Z),axis=0)\n",
        "            return (prob/sumprob)\n",
        "\n",
        "    def forward_propagation(self,Input):\n",
        "        Input = np.array(Input)\n",
        "        A = []\n",
        "        H = []\n",
        "        res = self.W[0].dot(Input) + self.b[0]\n",
        "        A.append(res)\n",
        "        number_of_layers =  self.Number_of_Layers\n",
        "        for i in range(1, number_of_layers):\n",
        "            H.append(self.activation(A[-1]))\n",
        "            preactivation = self.W[i].dot(H[-1]) + self.b[i]\n",
        "            A.append(preactivation)\n",
        "        y_hat = self.softmax_function(A[-1])\n",
        "        return A, H, y_hat\n",
        "\n",
        "    def backward_propagation(self, A, H, y_hat, y, Input, loss_type):\n",
        "        Input = np.array(Input)\n",
        "        H.insert(0,Input)\n",
        "        delW = []\n",
        "        delb = []\n",
        "        delA = []\n",
        "        delH = []\n",
        "\n",
        "        last_layer=self.Number_of_Neuron_each_Layer[-1]\n",
        "        zeros= np.zeros(last_layer)\n",
        "        ey = zeros\n",
        "        ey[y] = 1\n",
        "        \n",
        "        if loss_type==\"squared_error\":\n",
        "            der1=(y_hat - ey)\n",
        "            der2=(y_hat - y_hat**2)\n",
        "            delA.append(np.array(der1*der2))\n",
        "        else:\n",
        "        # delA and delH have reverse indexing\n",
        "            res=-(ey - y_hat)\n",
        "            delA.append(np.array(res))\n",
        "        number_of_layers= self.Number_of_Layers\n",
        "        for i in range( number_of_layers-1,-1,-1):\n",
        "            delastAshape=delA[-1].shape[0]\n",
        "            hlastshape=H[i].shape[0]\n",
        "            regulariz=self.L2reg_const*self.W[i]\n",
        "            delW.insert(0,delA[-1].reshape(delastAshape,1).dot(H[i].reshape(hlastshape,1).T) +regulariz )\n",
        "            delb.insert(0,delA[-1])\n",
        "            temp=self.W[i].T.dot(delA[-1])\n",
        "            delH.append(temp)\n",
        "            if i-1>=0:\n",
        "                delA.append(np.multiply(delH[-1], self.activation_derivative(A[i-1])))\n",
        "        return delW,delb\n",
        "    \n",
        "    \n",
        "    def initialize(self, Size_of_Input,Number_of_Layers,Number_of_Neuron_each_Layer):\n",
        "        W, b = [], []\n",
        "        each_layer_neuron = Number_of_Neuron_each_Layer[0]\n",
        "        input_size = Size_of_Input\n",
        "        W.append(np.zeros(( Number_of_Neuron_each_Layer[0],input_size )))\n",
        "        for i in range(1,Number_of_Layers):\n",
        "            curr_layer = Number_of_Neuron_each_Layer[i]\n",
        "            prev_layer = Number_of_Neuron_each_Layer[i-1]\n",
        "            W.append(np.zeros((curr_layer,prev_layer)))\n",
        "        for i in range(Number_of_Layers):\n",
        "            curr_layer = Number_of_Neuron_each_Layer[i]\n",
        "            b.append(np.zeros(curr_layer))            \n",
        "        return W, b\n",
        "\n",
        "    \n",
        "    def optimize(self, X, Y, val_images,val_labels,optimizer, learning_rate, max_epochs,batch_size,loss_type):\n",
        "        if optimizer == 'sgd':\n",
        "          self.stochastic_gradient_descent(X, Y, val_images,val_labels, learning_rate, max_epochs,loss_type)\n",
        "        if optimizer == 'momentum':\n",
        "          self.momentum_gradient_descent(X, Y, val_images,val_labels, learning_rate, max_epochs,batch_size,loss_type)\n",
        "        if optimizer == 'nag':\n",
        "          self.nesterov_accelerated_gradient_descent(X, Y, val_images,val_labels, learning_rate, max_epochs,batch_size,loss_type)\n",
        "        if optimizer == 'rmsprop':\n",
        "          self.rmsprop(X, Y, val_images,val_labels, learning_rate, max_epochs,batch_size,loss_type)\n",
        "        if optimizer == 'adam':\n",
        "          self.adam(X, Y, val_images,val_labels, learning_rate, max_epochs,batch_size,loss_type)\n",
        "        if optimizer == 'nadam':\n",
        "          self.nadam(X, Y, val_images,val_labels, learning_rate, max_epochs,batch_size,loss_type)\n",
        "\n",
        "\n",
        "    def stochastic_gradient_descent(self,X, Y, val_images,val_labels, learning_rate, max_epochs,loss_type):\n",
        "        number_of_inputs = self.Size_of_Input\n",
        "        layers_size = self.Number_of_Layers\n",
        "        neurons = self.Number_of_Neuron_each_Layer\n",
        "        for j in range(max_epochs):\n",
        "            correct = 0\n",
        "            error = 0\n",
        "            delW, delb = self.initialize(number_of_inputs,layers_size,neurons)\n",
        "            xsize=X.shape[0]\n",
        "            for i in range(xsize):\n",
        "                features=X[i]\n",
        "                A,H,y_hat = self.forward_propagation(features)\n",
        "                upW=self.W\n",
        "                s = [x.sum() for x in upW]\n",
        "                zeros=np.zeros(self.Number_of_Neuron_each_Layer[-1])\n",
        "                ey = zeros\n",
        "                ey[Y[i]] = 1\n",
        "                if loss_type == \"squared_error\":\n",
        "                      sqrd = 0.5*np.sum((ey-y_hat)**2)\n",
        "                      reg =  self.L2reg_const/2*sum(s)\n",
        "                      error += sqrd + reg \n",
        "                else:\n",
        "                      cross =-math.log(y_hat[Y[i]])\n",
        "                      reg = self.L2reg_const/2*sum(s)\n",
        "                      error += cross + reg\n",
        "\n",
        "                delW,delb = self.backward_propagation(A,H,y_hat,Y[i],X[i],loss_type)\n",
        "                res=np.argmax(y_hat)\n",
        "                if(res == Y[i]):\n",
        "                    correct +=1\n",
        "                \n",
        "                for i in range(self.Number_of_Layers):\n",
        "                    lrdelw = learning_rate*delW[i]\n",
        "                    self.W[i] = self.W[i] - lrdelw\n",
        "                    lrdelb = learning_rate*delb[i]\n",
        "                    self.b[i] = self.b[i] - lrdelb\n",
        "\n",
        "            v_error, v_accruracy = self.val_loss_and_accuracy(val_images, val_labels,loss_type)\n",
        "            sizex=X.shape[0]\n",
        "            error =error/sizex\n",
        "            sizexacc= X.shape[0]\n",
        "            accuracy = correct/sizexacc*100\n",
        "            wandb.log({'epoch' : j, 'train_loss' : error, 'train_accuracy' : accuracy,'valid_loss' : v_error,'valid_accuracy' : v_accruracy})\n",
        "\n",
        "\n",
        "    def momentum_gradient_descent(self,X, Y, val_images,val_labels, learning_rate, max_epochs,batch_size, loss_type,gamma = 0.6):\n",
        "        number_of_input=self.Size_of_Input\n",
        "        number_of_layers=self.Number_of_Layers\n",
        "        neuron=self.Number_of_Neuron_each_Layer\n",
        "        updateW, updateb = self.initialize(number_of_input,number_of_layers,neuron)\n",
        "\n",
        "        for j in range(max_epochs):\n",
        "            correct = 0\n",
        "            error = 0\n",
        "\n",
        "            delW, delb = self.initialize(number_of_input, number_of_layers,neuron)\n",
        "            xsize=X.shape[0]\n",
        "            \n",
        "            for i in range(X.shape[0]):\n",
        "                features=X[i]\n",
        "                A,H,y_hat = self.forward_propagation(features)\n",
        "                upW=self.W\n",
        "                s = [x.sum() for x in upW]\n",
        "                ey = np.zeros(self.Number_of_Neuron_each_Layer[-1])\n",
        "                ey[Y[i]] = 1\n",
        "                if loss_type == \"squared_error\":\n",
        "                      sqrd = 0.5*np.sum((ey-y_hat)**2)\n",
        "                      reg =  self.L2reg_const/2*sum(s)\n",
        "                      error += sqrd + reg \n",
        "                else : \n",
        "                     cross = -math.log(y_hat[Y[i]])\n",
        "                     reg = self.L2reg_const/2*sum(s)\n",
        "                     error += cross + reg\n",
        "                \n",
        "                w,b = self.backward_propagation(A,H,y_hat,Y[i],X[i],loss_type)\n",
        "\n",
        "                for k in range( number_of_layers):\n",
        "                    prevdelW = delW[k]\n",
        "                    delW[k]  = prevdelW + w[k]\n",
        "                    prevdelB = delb[k]\n",
        "                    delb[k]  = prevdelB + b[k]\n",
        "\n",
        "                for k in range( number_of_layers):\n",
        "                    gammaupW = gamma*updateW[k]\n",
        "                    lrdelw = learning_rate*delW[k]\n",
        "                    updateW[k] = gammaupW + learning_rate*delW[k]  \n",
        "\n",
        "                    gammaupB = gamma*updateb[k]\n",
        "                    lrdelb = learning_rate*delb[k]\n",
        "                    updateb[k] = gammaupB + lrdelb\n",
        "                temp=i%batch_size\n",
        "                if  (temp == 0 and i!=0) or i==xsize-1:\n",
        "                    delW, delb = self.initialize(number_of_input, number_of_layers,neuron)\n",
        "                    for k in range( number_of_layers):\n",
        "                        prevW1 = self.W[k] \n",
        "                        self.W[k] = prevW1 -updateW[k]\n",
        "                        prevB1 = self.b[k]  \n",
        "                        self.b[k] = prevB1 -updateb[k]\n",
        "                res = np.argmax(y_hat)\n",
        "                if(res == Y[i]):\n",
        "                    correct +=1\n",
        "\n",
        "                \n",
        "            v_error, v_accruracy = self.val_loss_and_accuracy(val_images, val_labels,loss_type)\n",
        "            sizex=X.shape[0]\n",
        "            error =error/sizex\n",
        "            sizexacc= X.shape[0]\n",
        "            accuracy = correct/sizexacc*100\n",
        "            wandb.log({'epoch' : j, 'train_loss' : error, 'train_accuracy' : accuracy,'valid_loss' : v_error,'valid_accuracy' : v_accruracy})\n",
        "\n",
        "\n",
        "    def nesterov_accelerated_gradient_descent(self, X, Y, val_images,val_labels, learning_rate, max_epochs,batch_size,loss_type ,gamma = 0.5):\n",
        "        number_of_input= self.Size_of_Input\n",
        "        layers_size=self.Number_of_Layers\n",
        "        neurons=self.Number_of_Neuron_each_Layer\n",
        "        updateW, updateb = self.initialize( number_of_input, layers_size,neurons)\n",
        "        lookaheadW, lookaheadb = self.initialize( number_of_input,  layers_size,neurons)\n",
        "        thetaW, thetab = self.initialize( number_of_input, layers_size,neurons)\n",
        "\n",
        "        for j in range(max_epochs):\n",
        "            correct = 0\n",
        "            error = 0\n",
        "\n",
        "            delW, delb = self.initialize(number_of_input,layers_size,neurons)\n",
        "                \n",
        "            for k in range(layers_size):\n",
        "                thetaW[k] = self.W[k]\n",
        "                thetab[k] = self.b[k]\n",
        "\n",
        "            for k in range(layers_size):\n",
        "                gammaupW=gamma*updateW[k]\n",
        "                lookaheadW[k] = thetaW[k] - gammaupW \n",
        "                gammaupB=gamma*updateb[k]\n",
        "                lookaheadb[k] = thetab[k] - gammaupB\n",
        "                self.W[k] = lookaheadW[k]\n",
        "                self.b[k] = lookaheadb[k]\n",
        "\n",
        "            xsize=X.shape[0]\n",
        "            for i in range(xsize):\n",
        "                A,H,y_hat = self.forward_propagation(X[i])\n",
        "                Wup=self.W\n",
        "                s = [x.sum() for x in Wup ]\n",
        "               # cross=-math.log(y_hat[Y[i]])\n",
        "               # regu= self.L2reg_const/2*sum(s)\n",
        "                #error += cross + regu\n",
        "                ey = np.zeros(self.Number_of_Neuron_each_Layer[-1])\n",
        "                ey[Y[i]] = 1\n",
        "                if loss_type == \"squared_error\":\n",
        "                      sqrd = 0.5*np.sum((ey-y_hat)**2)\n",
        "                      reg =  self.L2reg_const/2*sum(s)\n",
        "                      error += sqrd + reg \n",
        "                else : \n",
        "                    cross = -math.log(y_hat[Y[i]])\n",
        "                    reg = self.L2reg_const/2*sum(s)\n",
        "                    error += cross + reg\n",
        "                w,b = self.backward_propagation(A,H,y_hat,Y[i],X[i],loss_type)\n",
        "\n",
        "                for k in range(layers_size):\n",
        "                    prevdW=delW[k]\n",
        "                    delW[k] =prevdW + w[k]\n",
        "                    prevdB=delb[k]\n",
        "                    delb[k] =prevdB + b[k]\n",
        "\n",
        "                for k in range( layers_size):\n",
        "                    gammaW = gamma*updateW[k]\n",
        "                    lrdelW= learning_rate*delW[k] \n",
        "                    updateW[k] =  gammaW + lrdelW\n",
        "\n",
        "                    gammab =   gamma*updateb[k]\n",
        "                    lrdelb=    learning_rate*delb[k]\n",
        "                    updateb[k] = gammab +  lrdelb\n",
        "\n",
        "                temp=i%batch_size\n",
        "                if  (temp == 0 and i!=0) or i==xsize-1:\n",
        "                    delW, delb = self.initialize(self.Size_of_Input,layers_size,self.Number_of_Neuron_each_Layer)\n",
        "                    for k in range(layers_size):\n",
        "                        befW = self.W[k]\n",
        "                        self.W[k] = befW -updateW[k] \n",
        "                        befB = self.b[k]\n",
        "                        self.b[k] =befB -updateb[k]\n",
        "                res=np.argmax(y_hat)\n",
        "                if(res == Y[i]):\n",
        "                    correct +=1\n",
        "            \n",
        "           # error /= X.shape[0]\n",
        "           # accuracy = correct/X.shape[0]*100\n",
        "           # v_error, v_accruracy = self.val_loss_and_accuracy(val_images, val_labels)\n",
        "            v_error, v_accruracy = self.val_loss_and_accuracy(val_images, val_labels,loss_type)\n",
        "            sizex=X.shape[0]\n",
        "            error =error/sizex\n",
        "            sizexacc= X.shape[0]\n",
        "            accuracy = correct/sizexacc*100\n",
        "            wandb.log({'epoch' : j, 'train_loss' : error, 'train_accuracy' : accuracy,'valid_loss' : v_error,'valid_accuracy' : v_accruracy})\n",
        "\n",
        "\n",
        "\n",
        "    def rmsprop(self,X, Y, val_images,val_labels, learning_rate, max_epochs,batch_size,loss_type, beta = 0.89, epsilon = 1e-6):\n",
        "        number_of_inputs=self.Size_of_Input\n",
        "        layers_size=self.Number_of_Layers\n",
        "        neurons=self.Number_of_Neuron_each_Layer\n",
        "        v_W, v_b = self.initialize(number_of_inputs,layers_size,neurons)\n",
        "\n",
        "        for j in range(max_epochs):\n",
        "            error = 0\n",
        "            correct = 0\n",
        "            xsize=X.shape[0]\n",
        "\n",
        "            delW, delb = self.initialize(number_of_inputs,layers_size,neurons)\n",
        "    \n",
        "            for i in range(xsize):\n",
        "                val= X[i]\n",
        "                A,H,y_hat = self.forward_propagation(val)\n",
        "                Wup=self.W\n",
        "                s = [x.sum() for x in Wup ]\n",
        "                #cross= -math.log(y_hat[Y[i]])\n",
        "               # reg= self.L2reg_const/2*sum(s)\n",
        "                #error += cross  + reg\n",
        "                ey = np.zeros(self.Number_of_Neuron_each_Layer[-1])\n",
        "                ey[Y[i]] = 1\n",
        "                if loss_type == \"squared_error\":\n",
        "                      sqrd = 0.5*np.sum((ey-y_hat)**2)\n",
        "                      reg =  self.L2reg_const/2*sum(s)\n",
        "                      error += sqrd + reg \n",
        "                else : \n",
        "                    cross = -math.log(y_hat[Y[i]])\n",
        "                    reg = self.L2reg_const/2*sum(s)\n",
        "                    error += cross + reg\n",
        "\n",
        "                w,b = self.backward_propagation(A,H,y_hat,Y[i],X[i],loss_type)\n",
        "\n",
        "                for k in range(layers_size):\n",
        "                    prevdelW = delW[k]\n",
        "                    delW[k]  = prevdelW + w[k]\n",
        "                    prevdelB = delb[k]\n",
        "                    delb[k]  = prevdelB +  b[k]\n",
        "                res=np.argmax(y_hat)\n",
        "                if(res == Y[i]):\n",
        "                    correct +=1\n",
        "\n",
        "                for k in range(layers_size):\n",
        "                    betavw1=beta*v_W[k]\n",
        "                    betavw2=(1-beta)*delW[k]**2 \n",
        "                    v_W[k] = betavw1  + betavw2 \n",
        "\n",
        "                    betavb1=  beta*v_b[k]\n",
        "                    betavb2=  (1-beta)*delb[k]**2 \n",
        "                    v_b[k] = betavb1 + betavb2\n",
        "         \n",
        "                temp= i%batch_size\n",
        "                if  (temp== 0 and i!=0) or i==xsize-1:\n",
        "                    for k in range(layers_size):\n",
        "                        betavw1= beta*v_W[k]\n",
        "                        betavw2= (1-beta)*delW[k]**2  \n",
        "                        v_W[k] =  betavw1 +  betavw2\n",
        "\n",
        "                        betavb1=  beta*v_b[k]   \n",
        "                        betavb2=  (1-beta)*delb[k]**2\n",
        "                        v_b[k] = betavb1 + betavb2\n",
        "                    for k in range(layers_size):\n",
        "                        lrdelw=(learning_rate*delW[k])\n",
        "                        sqrtvW=np.sqrt(v_W[k] + epsilon)\n",
        "                        self.W[k] = self.W[k] - lrdelw/sqrtvW\n",
        "\n",
        "                        lrdelb=(learning_rate*delb[k])\n",
        "                        sqrtvB=np.sqrt(v_b[k] + epsilon)\n",
        "                        self.b[k] = self.b[k] - lrdelb/sqrtvB\n",
        "                    delW, delb = self.initialize(number_of_inputs,layers_size,neurons) \n",
        "            \n",
        "\n",
        "            v_error, v_accruracy = self.val_loss_and_accuracy(val_images, val_labels,loss_type)\n",
        "            sizex=X.shape[0]\n",
        "            error =error/sizex\n",
        "            sizexacc= X.shape[0]\n",
        "            accuracy = correct/sizexacc*100\n",
        "            wandb.log({'epoch' : j, 'train_loss' : error, 'train_accuracy' : accuracy,'valid_loss' : v_error,'valid_accuracy' : v_accruracy})\n",
        "\n",
        "\n",
        "    \n",
        "    def adam(self,X, Y, val_images,val_labels, learning_rate, max_epochs,batch_size, loss_type,beta1 = 0.89,beta2 = 0.989,epsilon = 1e-8):\n",
        "        number_of_input = self.Size_of_Input\n",
        "        number_of_layers= self.Number_of_Layers\n",
        "        neurons= self.Number_of_Neuron_each_Layer\n",
        "        m_W, m_b = self.initialize( number_of_input,number_of_layers,neurons)\n",
        "        m_hat_W, m_hat_b = self.initialize(  number_of_input,number_of_layers,neurons)\n",
        "        v_W, v_b = self.initialize( number_of_input,number_of_layers,neurons)\n",
        "        v_hat_W, v_hat_b = self.initialize( number_of_input,number_of_layers,neurons)\n",
        "        \n",
        "        for j in range(0, max_epochs):\n",
        "            correct = 0\n",
        "            error = 0\n",
        "            xsize = X.shape[0]\n",
        "            delW, delb = self.initialize( number_of_input,number_of_layers,neurons)\n",
        "            \n",
        "            for i in range(xsize):\n",
        "                features=X[i]\n",
        "                A,H,y_hat = self.forward_propagation(features)\n",
        "                upW = self.W\n",
        "                s = [x.sum() for x in upW]\n",
        "               # cross = -math.log(y_hat[Y[i]])\n",
        "                #reg = self.L2reg_const/2*sum(s)\n",
        "               # error += cross + reg\n",
        "                ey = np.zeros(self.Number_of_Neuron_each_Layer[-1])\n",
        "                ey[Y[i]] = 1\n",
        "                if loss_type == \"squared_error\":\n",
        "                      sqrd = 0.5*np.sum((ey-y_hat)**2)\n",
        "                      reg =  self.L2reg_const/2*sum(s)\n",
        "                      error += sqrd + reg \n",
        "                else : \n",
        "                    cross = -math.log(y_hat[Y[i]])\n",
        "                    reg = self.L2reg_const/2*sum(s)\n",
        "                    error += cross + reg\n",
        "               \n",
        "\n",
        "                w,b = self.backward_propagation(A,H,y_hat,Y[i],X[i],loss_type)\n",
        "\n",
        "                for k in range(number_of_layers):\n",
        "                    prevdelW = delW[k]\n",
        "                    delW[k] = prevdelW + w[k]\n",
        "                    prevdelB = delb[k]\n",
        "                    delb[k] = prevdelB + b[k]\n",
        "                res = np.argmax(y_hat) \n",
        "                if(res == Y[i]):\n",
        "                    correct +=1\n",
        "                temp = i%batch_size\n",
        "                if  (temp == 0 and i!=0) or i==xsize-1:\n",
        "                    for k in range(number_of_layers):\n",
        "                        betavW1 =  beta2*v_W[k]\n",
        "                        betavW2 = (1-beta2)*delW[k]*delW[k]\n",
        "                        v_W[k] =   betavW1 + betavW2\n",
        "\n",
        "                        betavB1 = beta2*v_b[k]\n",
        "                        betavB2=  (1-beta2)*delb[k]*delb[k]\n",
        "                        v_b[k] =  betavB1 + betavB2\n",
        "\n",
        "                        betamW1 = beta1*m_W[k]\n",
        "                        betadelw =  (1-beta1)*delW[k]\n",
        "                        m_W[k] = betamW1 + betadelw\n",
        "\n",
        "                        betamb1 = beta1*m_b[k]\n",
        "                        betadelb= beta1*m_b[k]\n",
        "                        m_b[k] = betamb1 + betadelb\n",
        "\n",
        "                        powbeta1= (math.pow(beta1,j))\n",
        "                        m_hat_W[k] = m_W[k]/ powbeta1\n",
        "                        m_hat_b[k] = m_b[k]/powbeta1\n",
        "\n",
        "                        powbeta2= (math.pow(beta2,j))\n",
        "                        v_hat_W[k] = v_W[k]/powbeta2\n",
        "                        v_hat_b[k] = v_b[k]/powbeta2\n",
        "                    \n",
        "                    for k in range(number_of_layers):\n",
        "                        lrW = learning_rate*m_hat_W[k]\n",
        "                        sqrtvW = np.sqrt(v_hat_W[k] + epsilon) \n",
        "                        self.W[k] = self.W[k] - (lrW)/sqrtvW\n",
        "\n",
        "                        lrb = learning_rate*m_hat_b[k]\n",
        "                        sqrtvb = np.sqrt(v_hat_b[k] + epsilon)\n",
        "                        self.b[k] = self.b[k] - (lrb)/sqrtvb\n",
        "                    delW, delb = self.initialize(number_of_input,number_of_layers,neurons)\n",
        "                                \n",
        "            v_error, v_accruracy = self.val_loss_and_accuracy(val_images, val_labels,loss_type)\n",
        "            sizex=X.shape[0]\n",
        "            error =error/sizex\n",
        "            sizexacc= X.shape[0]\n",
        "            accuracy = correct/sizexacc*100\n",
        "            wandb.log({'epoch' : j, 'train_loss' : error, 'train_accuracy' : accuracy,'valid_loss' : v_error,'valid_accuracy' : v_accruracy})\n",
        "    \n",
        "    def nadam(self, X, Y, val_images,val_labels, learning_rate, max_epochs,batch_size, loss_type,beta1 = 0.89,beta2 = 0.989,epsilon = 1e-8):\n",
        "        number_of_inputs=self.Size_of_Input\n",
        "        layers_size=self.Number_of_Layers\n",
        "        neurons=self.Number_of_Neuron_each_Layer\n",
        "        v_W, v_b = self.initialize(number_of_inputs,layers_size,neurons)\n",
        "        v_hat_W, v_hat_b = self.initialize(number_of_inputs,layers_size,neurons)\n",
        "        m_W, m_b = self.initialize(number_of_inputs,layers_size,neurons)\n",
        "        m_hat_W, m_hat_b = self.initialize(number_of_inputs,layers_size,neurons)\n",
        "\n",
        "        \n",
        "        for j in range(1,max_epochs):\n",
        "            error = 0\n",
        "            correct = 0\n",
        "\n",
        "            sizex=X.shape[0]\n",
        "            delW, delb = self.initialize(number_of_inputs,self.Number_of_Layers,neurons)\n",
        "            \n",
        "            for i in range(sizex):\n",
        "                features=X[i]\n",
        "                A,H,y_hat = self.forward_propagation(features)\n",
        "                \n",
        "                s = [x.sum() for x in self.W]\n",
        "              # # crosse=-math.log(y_hat[Y[i]])\n",
        "                #regu=self.L2reg_const/2*sum(s)\n",
        "               # error += crosse + regu\n",
        "                ey = np.zeros(self.Number_of_Neuron_each_Layer[-1])\n",
        "                ey[Y[i]] = 1\n",
        "                if loss_type == \"squared_error\":\n",
        "                      sqrd = 0.5*np.sum((ey-y_hat)**2)\n",
        "                      reg =  self.L2reg_const/2*sum(s)\n",
        "                      error += sqrd + reg \n",
        "                else : \n",
        "                    cross = -math.log(y_hat[Y[i]])\n",
        "                    reg = self.L2reg_const/2*sum(s)\n",
        "                    error += cross + reg\n",
        "\n",
        "                w,b = self.backward_propagation(A,H,y_hat,Y[i],X[i],loss_type)\n",
        "\n",
        "                for k in range(layers_size):\n",
        "                    prevdelW= delW[k]\n",
        "                    delW[k] = prevdelW + w[k]\n",
        "                    prevdelB= delb[k]\n",
        "                    delb[k] = prevdelB+  b[k]\n",
        "                res=np.argmax(y_hat) \n",
        "                if(res== Y[i]):\n",
        "                    correct +=1\n",
        "                temp=i%batch_size\n",
        "\n",
        "                if  (temp == 0 and i!=0) or i==sizex-1:\n",
        "                    for k in range(layers_size):\n",
        "                        betavW1= beta2*v_W[k]\n",
        "                        betavW2= (1-beta2)*delW[k]**2\n",
        "                        v_W[k] =  betavW1 + betavW2\n",
        "\n",
        "                        betavB1=beta2*v_b[k]\n",
        "                        betavB2=(1-beta2)*delb[k]**2\n",
        "                        v_b[k] = betavB1  + betavB2 \n",
        "                        \n",
        "                        betamW1=beta1*m_W[k]\n",
        "                        betamW2=(1-beta1)*delW[k]\n",
        "                        m_W[k] = betamW1 + betamW2\n",
        "\n",
        "                        betamB1=beta1*m_b[k]\n",
        "                        betamB2=(1-beta1)*delb[k]\n",
        "                        m_b[k] = betamB1+ betamB2\n",
        "                        \n",
        "                        m_hat_bias= (math.pow(beta1,j))\n",
        "                        m_hat_W[k] = m_W[k]/m_hat_bias\n",
        "                        m_hat_b[k] = m_b[k]/m_hat_bias\n",
        "                        \n",
        "                        v_hat_bias = (math.pow(beta2,j))\n",
        "                        v_hat_W[k] = v_W[k]/v_hat_bias\n",
        "                        v_hat_b[k] = v_b[k]/v_hat_bias\n",
        "                    for k in range(self.Number_of_Layers):\n",
        "                        beta_mw=beta1*m_hat_W[k]\n",
        "                        beta_dw=(1-beta1)*delW[k]/(1-beta1)\n",
        "                        sqrt_dw=np.sqrt(v_hat_W[k] + epsilon)\n",
        "                        self.W[k] = self.W[k] - (learning_rate*(beta_mw + beta_dw))/sqrt_dw\n",
        "\n",
        "                        beta_mb=beta1*m_hat_b[k]\n",
        "                        beta_db=(1-beta1)*delb[k]/(1-beta1)\n",
        "                        sqrt_db=np.sqrt(v_hat_b[k] + epsilon)\n",
        "                        self.b[k] = self.b[k] - (learning_rate*(beta_mb+beta_db))/sqrt_db\n",
        "                    delW, delb = self.initialize(self.Size_of_Input,self.Number_of_Layers,self.Number_of_Neuron_each_Layer)\n",
        "                    \n",
        "\n",
        "            v_error, v_accruracy = self.val_loss_and_accuracy(val_images, val_labels,loss_type)\n",
        "            sizex=X.shape[0]\n",
        "            error =error/sizex\n",
        "            sizexacc= X.shape[0]\n",
        "            accuracy = correct/sizexacc*100\n",
        "            wandb.log({'epoch' : j, 'train_loss' : error, 'train_accuracy' : accuracy,'valid_loss' : v_error,'valid_accuracy' : v_accruracy})\n",
        "        \n",
        "    \n",
        "    def val_loss_and_accuracy(self,val_data,val_labels,loss_type):\n",
        "        val_loss = []\n",
        "        val_accuracy = []\n",
        "        val_correct = 0\n",
        "        val_error = 0\n",
        "\n",
        "        for i in range(val_data.shape[0]):\n",
        "            A,H,y_hat = self.forward_propagation(val_data[i]) \n",
        "            upW=self.W\n",
        "            s = [x.sum() for x in upW]\n",
        "            if loss_type == \"squared_error\":\n",
        "                ey = np.zeros(self.Number_of_Neuron_each_Layer[-1])\n",
        "                ey[val_labels[i]] = 1\n",
        "                error=0.5*np.sum((ey-y_hat)**2)\n",
        "                regerror= self.L2reg_const/2*sum(s)\n",
        "                val_error += error + regerror\n",
        "            else:\n",
        "                 regu=self.L2reg_const/2*sum(s)\n",
        "                 cross=-math.log(y_hat[val_labels[i]])\n",
        "                 val_error += cross + regu\n",
        "            maxi=np.argmax(y_hat)\n",
        "            if  maxi == val_labels[i]:\n",
        "                val_correct += 1\n",
        "        m=val_data.shape[0]\n",
        "        m1=val_data.shape[0]\n",
        "        return val_error/m, val_correct/m1*100\n",
        "\n",
        "\n",
        "    def test(self,test_data,test_labels):\n",
        "        correct = 0\n",
        "        y_hat = []\n",
        "        testsize=test_data.shape[0]\n",
        "        for i in range(testsize):\n",
        "\n",
        "            A,H,y = self.forward_propagation(test_data[i])\n",
        "            res=np.argmax(y)\n",
        "            if res == test_labels[i]:\n",
        "                correct += 1\n",
        "            y_hat.append(y)\n",
        "        a=np.argmax(np.array(y_hat),axis=1)\n",
        "        b=correct/test_data.shape[0]*100\n",
        "        return a,b"
      ],
      "metadata": {
        "id": "G8WCQ8WseC1F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "xsize=784\n",
        "output_size=10\n",
        "Batch_size=16\n",
        "hidden_layer=[32,16,10]\n",
        "number_of_hidden_layer=3\n",
        "obj = NeuralNet(xsize, hidden_layer, number_of_hidden_layer, 'ReLU', 'xavier', 0)\n",
        "optimizer='nadam'\n",
        "obj.optimize(normalized, train_labels, val_images, val_labels, optimizer, 0.0003,output_size, Batch_size,\"cross_entropy\")\n",
        "y_pred, test_accuracy = obj.test(test_images, test_labels)\n",
        "labels_all= [0,1,2,3,4,5,6,7,8,9]\n",
        "cm = cnfm(y_pred, test_labels, labels = labels_all)  \n",
        "value=False\n",
        "print(test_accuracy)\n",
        "#wandb.sklearn.plot_confusion_matrix(test_labels, y_pred, labels_all)\n",
        "#wandb.log({'Confusion matrix': wandb.plots.HeatMap(labels_all, labels_all, cm, show_text=value), 'test_accuracy' : test_accuracy})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J25T_xpReV8T",
        "outputId": "08d07a2e-731b-4465-bedb-832897163185"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "96.34\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "xsize=784\n",
        "output_size=10\n",
        "Batch_size=16\n",
        "hidden_layer=[128,128,10]\n",
        "number_of_hidden_layer=3\n",
        "obj = NeuralNet(xsize, hidden_layer, number_of_hidden_layer, 'ReLU', 'xavier', 0)\n",
        "optimizer='sgd'\n",
        "obj.optimize(normalized, train_labels, val_images, val_labels, optimizer, 0.01,output_size, Batch_size,\"cross_entropy\")\n",
        "y_pred, test_accuracy = obj.test(test_images, test_labels)\n",
        "labels_all= [0,1,2,3,4,5,6,7,8,9]\n",
        "cm = cnfm(y_pred, test_labels, labels = labels_all)  \n",
        "value=False\n",
        "print(test_accuracy)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j2bhzimOeqXH",
        "outputId": "67e751ae-9478-47e1-af11-07301b280f11"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "96.66\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "xsize=784\n",
        "output_size=10\n",
        "Batch_size=16\n",
        "hidden_layer=[128,128,10]\n",
        "number_of_hidden_layer=3\n",
        "obj = NeuralNet(xsize, hidden_layer, number_of_hidden_layer, 'ReLU', 'xavier', 0)\n",
        "optimizer='rmsprop'\n",
        "obj.optimize(normalized, train_labels, val_images, val_labels, optimizer, 0.01,output_size, Batch_size,\"cross_entropy\")\n",
        "y_pred, test_accuracy = obj.test(test_images, test_labels)\n",
        "labels_all= [0,1,2,3,4,5,6,7,8,9]\n",
        "cm = cnfm(y_pred, test_labels, labels = labels_all)  \n",
        "value=False\n",
        "print(test_accuracy)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "No0q00OMgIs3",
        "outputId": "1655643a-8b57-4310-c588-a106c0f24ed9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "91.36999999999999\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "WIV_PedAhXxV"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "a22208e638ac48279d23273b12670d4c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "VBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "VBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "VBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_2c22d6bbaea3480b83dc6d792b821290",
              "IPY_MODEL_571211911d7d4669bd39cee480f13f71"
            ],
            "layout": "IPY_MODEL_bcbc7a2cf66e4885ad123040ca9258a7"
          }
        },
        "2c22d6bbaea3480b83dc6d792b821290": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "LabelModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "LabelModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "LabelView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_fcec2d4cbaf346a5a56cc7a28c8f973a",
            "placeholder": "​",
            "style": "IPY_MODEL_5ff680f9048c45548160cc30bb2449cc",
            "value": "0.006 MB of 0.014 MB uploaded (0.000 MB deduped)\r"
          }
        },
        "571211911d7d4669bd39cee480f13f71": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e16cbadde22d45de9f9d1487c606b4ee",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_1439ca20a2dc4d5b8c175828ac2fb32d",
            "value": 0.42864696939652325
          }
        },
        "bcbc7a2cf66e4885ad123040ca9258a7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fcec2d4cbaf346a5a56cc7a28c8f973a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5ff680f9048c45548160cc30bb2449cc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "e16cbadde22d45de9f9d1487c606b4ee": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1439ca20a2dc4d5b8c175828ac2fb32d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "19e63f3f05b34249b12061aff63a6d82": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "VBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "VBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "VBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_fa66be212a0b45ee88c0922b15f052e3",
              "IPY_MODEL_e697f544c65149a1a0ba55c1968e90a3"
            ],
            "layout": "IPY_MODEL_0247d18e7cea4eb7a9301e838bdbf45e"
          }
        },
        "fa66be212a0b45ee88c0922b15f052e3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "LabelModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "LabelModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "LabelView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_80ccdbdd0b3541eb96635096d6b01f37",
            "placeholder": "​",
            "style": "IPY_MODEL_61925964b39c424d98e06cdfd0801929",
            "value": "0.001 MB of 0.008 MB uploaded (0.000 MB deduped)\r"
          }
        },
        "e697f544c65149a1a0ba55c1968e90a3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b06a61c06d5c452cb2cd8924e96fd819",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_f29d00ce976e42c489aa914094e8dc00",
            "value": 0.11843328684332868
          }
        },
        "0247d18e7cea4eb7a9301e838bdbf45e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "80ccdbdd0b3541eb96635096d6b01f37": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "61925964b39c424d98e06cdfd0801929": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "b06a61c06d5c452cb2cd8924e96fd819": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f29d00ce976e42c489aa914094e8dc00": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}